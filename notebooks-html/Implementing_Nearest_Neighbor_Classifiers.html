<div id="ipython-notebook">
            <a class="interact-button" href="http://datahub.berkeley.edu/user-redirect/interact?repo=textbook&path=notebooks/wine.csv&path=notebooks/Implementing_Nearest_Neighbor_Classifiers.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Implementing-Nearest-Neighbor-Classifiers">Implementing Nearest Neighbor Classifiers<a class="anchor-link" href="#Implementing-Nearest-Neighbor-Classifiers">¶</a></h3><p>Do this one with the wine data. Text will need to be adjusted accordingly.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Applying-the-k-nearest-neighbor-classifier-to-breast-cancer-diagnosis">Applying the k-nearest neighbor classifier to breast cancer diagnosis<a class="anchor-link" href="#Applying-the-k-nearest-neighbor-classifier-to-breast-cancer-diagnosis">¶</a></h3><p>We've got a data set.  Let's try out the $k$-nearest neighbor classifier and see how it does.  This is going to be great.</p>
<p>We're going to need an implementation of the $k$-nearest neighbor classifier.  In practice you would probably use an existing library, but it's simple enough that I'm going to imeplment it myself.</p>
<p>The first thing we need is a way to compute the distance between two points.  How do we do this?  In 2-dimensional space, it's pretty easy.  If we have a point at coordinates $(x_0,y_0)$ and another at $(x_1,y_1)$, the distance between them is</p>
$$D = \sqrt{(x_0-x_1)^2 + (y_0-y_1)^2}.$$<p>(Where did this come from?  It comes from the Pythogorean theorem: we have a right triangle with side lengths $x_0-x_1$ and $y_0-y_1$, and we want to find the length of the diagonal.)</p>
<p>In 3-dimensional space, the formula is</p>
$$D = \sqrt{x_0-x_1)^2 + (y_0-y_1)^2 + (z_0-z_1)^2}.$$<p>In $k$-dimensional space, things are a bit harder to visualize, but I think you can see how the formula generalized: we sum up the squares of the differences between each individual coordinate, and then take the square root of that.  Let's implement a function to compute this distance function for us:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">pt1</span><span class="p">,</span> <span class="n">pt2</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pt1</span><span class="p">)):</span>
        <span class="n">total</span> <span class="o">=</span> <span class="n">total</span> <span class="o">+</span> <span class="p">(</span><span class="n">pt1</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">-</span> <span class="n">pt2</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we're going to write some code to implement the classifier.  The input is a patient <code>p</code> who we want to diagnose.  The classifier works by finding the $k$ nearest neighbors of <code>p</code> from the training set.  So, our approach will go like this:</p>
<ol>
<li><p>Find the closest $k$ neighbors of <code>p</code>, i.e., the $k$ patients from the training set that are most similar to <code>p</code>.</p>
</li>
<li><p>Look at the diagnoses of those $k$ neighbors, and take the majority vote to find the most-common diagnosis.  Use that as our predicted diagnosis for <code>p</code>.</p>
</li>
</ol>
<p>So that will guide the structure of our Python code.</p>
<p>To implement the first step, we will compute the distance from each patient in the training set to <code>p</code>, sort them by distance, and take the $k$ closest patients in the training set.  The code will make a copy of the table, compute the distance from each patient to <code>p</code>, add a new column to the table with those distances, and then sort the table by distance and take the first $k$ rows.  That leads to the following Python code:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="o">...</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">kclosest</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">kclosest</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">kclosest</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">kclosest</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">training</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">,</span> <span class="n">dists</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">withdists</span> <span class="o">=</span> <span class="n">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">sortedbydist</span> <span class="o">=</span> <span class="n">withdists</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sortedbydist</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topk</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">&gt;</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">topkclasses</span> <span class="o">=</span> <span class="n">closestk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see how this works, with our data set.  We'll take patient 12 and imagine we're going to try to diagnose them:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">patients</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">'breast-cancer.csv'</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'ID'</span><span class="p">)</span>
<span class="n">patients</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Clump Thickness</th> <th>Uniformity of Cell Size</th> <th>Uniformity of Cell Shape</th> <th>Marginal Adhesion</th> <th>Single Epithelial Cell Size</th> <th>Bare Nuclei</th> <th>Bland Chromatin</th> <th>Normal Nucleoli</th> <th>Mitoses</th> <th>Class</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>5              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>4                      </td> <td>4                       </td> <td>5                </td> <td>7                          </td> <td>10         </td> <td>3              </td> <td>2              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>2          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>6              </td> <td>8                      </td> <td>8                       </td> <td>1                </td> <td>3                          </td> <td>4          </td> <td>3              </td> <td>7              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>4              </td> <td>1                      </td> <td>1                       </td> <td>3                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>8              </td> <td>10                     </td> <td>10                      </td> <td>8                </td> <td>7                          </td> <td>10         </td> <td>9              </td> <td>7              </td> <td>1      </td> <td>1    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>10         </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>2              </td> <td>1                      </td> <td>2                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>3              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>2              </td> <td>1                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>1              </td> <td>1              </td> <td>5      </td> <td>0    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>4              </td> <td>2                      </td> <td>1                       </td> <td>1                </td> <td>2                          </td> <td>1          </td> <td>2              </td> <td>1              </td> <td>1      </td> <td>0    </td>
        </tr>
    </tbody>
</table>
<p>... (673 rows omitted)</p></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">patients</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Clump Thickness</th> <th>Uniformity of Cell Size</th> <th>Uniformity of Cell Shape</th> <th>Marginal Adhesion</th> <th>Single Epithelial Cell Size</th> <th>Bare Nuclei</th> <th>Bland Chromatin</th> <th>Normal Nucleoli</th> <th>Mitoses</th> <th>Class</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>3                </td> <td>2                          </td> <td>3          </td> <td>4              </td> <td>4              </td> <td>1      </td> <td>1    </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can pull out just their attributes (excluding the class), like this:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">patients</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>Row(Clump Thickness=5, Uniformity of Cell Size=3, Uniformity of Cell Shape=3, Marginal Adhesion=3, Single Epithelial Cell Size=2, Bare Nuclei=3, Bland Chromatin=4, Normal Nucleoli=4, Mitoses=1)</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's take $k=5$.  We can find the 5 nearest neighbors:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">closest</span><span class="p">(</span><span class="n">patients</span><span class="p">,</span> <span class="n">patients</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Clump Thickness</th> <th>Uniformity of Cell Size</th> <th>Uniformity of Cell Shape</th> <th>Marginal Adhesion</th> <th>Single Epithelial Cell Size</th> <th>Bare Nuclei</th> <th>Bland Chromatin</th> <th>Normal Nucleoli</th> <th>Mitoses</th> <th>Class</th> <th>Distance</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>3                </td> <td>2                          </td> <td>3          </td> <td>4              </td> <td>4              </td> <td>1      </td> <td>1    </td> <td>0       </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>4                </td> <td>2                          </td> <td>4          </td> <td>3              </td> <td>4              </td> <td>1      </td> <td>1    </td> <td>1.73205 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>1                      </td> <td>3                       </td> <td>3                </td> <td>2                          </td> <td>2          </td> <td>2              </td> <td>3              </td> <td>1      </td> <td>0    </td> <td>3.16228 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>2                      </td> <td>2                       </td> <td>2                </td> <td>2                          </td> <td>2          </td> <td>3              </td> <td>2              </td> <td>2      </td> <td>0    </td> <td>3.16228 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>1                </td> <td>3                          </td> <td>3          </td> <td>3              </td> <td>3              </td> <td>3      </td> <td>1    </td> <td>3.31662 </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>3 out of the 5 nearest neighbors have class 1, so the majority is 1 (has cancer) -- and that is the output of our classifier for this patient:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classify</span><span class="p">(</span><span class="n">patients</span><span class="p">,</span> <span class="n">patients</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>1</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Awesome!  We now have a classification algorithm for diagnosing whether a patient has breast cancer or not, based on the measurements from the lab.  Are we done?  Shall we give this to doctors to use?</p>
<p>Hold on: we're not done yet.  There's an obvious question to answer, before we start using this in practice:</p>
<p><em>How accurate is this method, at diagnosing breast cancer?</em></p>
<p>And that raises a more fundamental issue.  How can we measure the accuracy of a classification algorithm?</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">""" REDONE STUFF"""</span>

<span class="k">def</span> <span class="nf">distance2</span><span class="p">(</span><span class="n">pt1</span><span class="p">,</span> <span class="n">pt2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">pt1</span> <span class="o">-</span> <span class="n">pt2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">distance_from_individual</span><span class="p">(</span><span class="n">attribute_table</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="sd">"""Need to explain .row,</span>
<span class="sd">    and that we can make it an array here as it's all numerical"""</span>
    <span class="k">return</span> <span class="n">distance2</span><span class="p">(</span><span class="n">make_array</span><span class="p">(</span><span class="n">attribute_table</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="n">i</span><span class="p">)),</span> <span class="n">p</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">table_with_dists2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="sd">"""Redone to resemble other such code in the course."""</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">distance_from_individual</span><span class="p">(</span><span class="n">attributes</span><span class="p">,</span> <span class="n">i</span> <span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">training</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">,</span> <span class="n">dists</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">closest2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">with_dists</span> <span class="o">=</span> <span class="n">table_with_dists2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">sorted_by_dist</span> <span class="o">=</span> <span class="n">with_dists</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sorted_by_dist</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topk</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="n">ones</span> <span class="o">=</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="n">are</span><span class="o">.</span><span class="n">equal_to</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">num_rows</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="n">are</span><span class="o">.</span><span class="n">equal_to</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">num_rows</span>
    <span class="k">if</span> <span class="n">ones</span> <span class="o">&gt;</span> <span class="n">zeros</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">topkclasses</span> <span class="o">=</span> <span class="n">closestk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">point</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">(</span><span class="n">patients</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span>
<span class="n">point</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>array([[5, 3, 3, 3, 2, 3, 4, 4, 1]])</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">closest2</span><span class="p">(</span><span class="n">patients</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Clump Thickness</th> <th>Uniformity of Cell Size</th> <th>Uniformity of Cell Shape</th> <th>Marginal Adhesion</th> <th>Single Epithelial Cell Size</th> <th>Bare Nuclei</th> <th>Bland Chromatin</th> <th>Normal Nucleoli</th> <th>Mitoses</th> <th>Class</th> <th>Distance</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>3                </td> <td>2                          </td> <td>3          </td> <td>4              </td> <td>4              </td> <td>1      </td> <td>1    </td> <td>0       </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>4                </td> <td>2                          </td> <td>4          </td> <td>3              </td> <td>4              </td> <td>1      </td> <td>1    </td> <td>1.73205 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>1                      </td> <td>3                       </td> <td>3                </td> <td>2                          </td> <td>2          </td> <td>2              </td> <td>3              </td> <td>1      </td> <td>0    </td> <td>3.16228 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>2                      </td> <td>2                       </td> <td>2                </td> <td>2                          </td> <td>2          </td> <td>3              </td> <td>2              </td> <td>2      </td> <td>0    </td> <td>3.16228 </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>5              </td> <td>3                      </td> <td>3                       </td> <td>1                </td> <td>3                          </td> <td>3          </td> <td>3              </td> <td>3              </td> <td>3      </td> <td>1    </td> <td>3.31662 </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classify2</span><span class="p">(</span><span class="n">patients</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>1</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">patients</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>Row(Clump Thickness=5, Uniformity of Cell Size=3, Uniformity of Cell Shape=3, Marginal Adhesion=3, Single Epithelial Cell Size=2, Bare Nuclei=3, Bland Chromatin=4, Normal Nucleoli=4, Mitoses=1, Class=1)</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Measuring-accuracy-of-a-classifier">Measuring accuracy of a classifier<a class="anchor-link" href="#Measuring-accuracy-of-a-classifier">¶</a></h3><p>We've got a classifier, and we'd like to determine how accurate it will be.  How can we measure that?</p>
<p><strong>Try it out.</strong>  One natural idea is to just try it on patients for a year, keep records on it, and see how accurate it is.  However, this has some disadvantages: (a) we're trying something on patients without knowing how accurate it is, which might be unethical; (b) we have to wait a year to find out whether our classifier is any good.  If it's not good enough and we get an idea for an improvement, we'll have to wait another year to find out whether our improvement was better.</p>
<p><strong>Get some more data.</strong> We could try to get some more data from other patients whose diagnosis is known, and measure how accurate our classifier's predictions are on those additional patients.  We can compare what the classifier outputs against what we know to be true.</p>
<p><strong>Use the data we already have.</strong> Another natural idea is to re-use the data we already have: we have a training set that we used to train our classifier, so we could just run our classifier on every patient in the data set and compare what it outputs to what we know to be true.  This is sometimes known as testing the classifier on your training set.</p>
<p>How should we choose among these options?  Are they all equally good?</p>
<p>It turns out that the third option, testing the classifier on our training set, is fundamentally flawed.  It might sound attractive, but it gives misleading results: it will over-estimate the accuracy of the classifier (it will make us think the classifier is more accurate than it really is).  Intuitively, the problem is that what we really want to know is how well the classifier has done at "generalizing" beyond the specific examples in the training set; but if we test it on patients from the training set, then we haven't learned anything about how well it would generalize to other patients.</p>
<p>This is subtle, so it might be helpful to try an example.  Let's try a thought experiment.  Let's focus on the 1-nearest neighbor classifier ($k=1$).  Suppose you trained the 1-nearest neighbor classifier on data from all 683 patients in the data set, and then you tested it on those same 683 patients.  How many would it get right?  Think it through and see if you can work out what will happen.  That's right!  The classifier will get the right answer for all 683 patients.  Suppose we apply the classifier to a patient from the training set, say Alice.  The classifier will look for the nearest neighbor (the most similar patient from the training set), and the nearest neighbor will turn out to be Alice herself (the distance from any point to itself is zero).  Thus, the classifier will produce the right diagnosis for Alice.  The same reasoning applies to every other patient in the training set.</p>
<p>So, if we test the 1-nearest neighbor classifier on the training set, the accuracy will always be 100%: absolutely perfect.  This is true no matter whether there are actually any patterns in the data.  But the 100% is a total lie.  When you apply the classifier to other patients who were not in the training set, the accuracy could be far worse.</p>
<p>In other words, testing on the training tells you nothing about how accurate the 1-nearest neighbor classifier will be.  This illustrates why testing on the training set is so flawed.  This flaw is pretty blatant when you use the 1-nearest neighbor classifier, but don't think that with some other classifier you'd be immune to this problem -- the problem is fundamental and applies no matter what classifier you use.  Testing on the training set gives you a biased estimate of the classifier's accurate.  For these reasons, you should never test on the training set.</p>
<p>So what <em>should</em> you do, instead?  Is there a more principled approach?</p>
<p>It turns out there is.  The approach comes down to: get more data.  More specifically, the right solution is to use one data set for training, and a different data set for testing, with no overlap between the two data sets.  We call these a <em>training set</em> and a <em>test set</em>.</p>
<p>Where do we get these two data sets from?  Typically, we'll start out with some data, e.g., the data set on 683 patients, and before we do anything else with it, we'll split it up into a training set and a test set.  We might put 50% of the data into the training set and the other 50% into the test set.  Basically, we are setting aside some data for later use, so we can use it to measure the accuracy of our classifier.  Sometimes people will call the data that you set aside for testing a <em>hold-out set</em>, and they'll call this strategy for estimating accuracy the <em>hold-out method</em>.</p>
<p>Note that this approach requires great discipline.  Before you start applying machine learning methods, you have to take some of your data and set it aside for testing.  You must avoid using the test set for developing your classifier: you shouldn't use it to help train your classifier or tweak its settings or for brainstorming ways to improve your classifier.  Instead, you should use it only once, at the very end, after you've finalized your classifier, when you want an unbiased estimate of its accuracy.</p>
<h2 id="The-effectiveness-of-our-classifier,-for-breast-cancer">The effectiveness of our classifier, for breast cancer<a class="anchor-link" href="#The-effectiveness-of-our-classifier,-for-breast-cancer">¶</a></h2><p>OK, so let's apply the hold-out method to evaluate the effectiveness of the $k$-nearest neighbor classifier for breast cancer diagnosis.  The data set has 683 patients, so we'll randomly permute the data set and put 342 of them in the training set and the remaining 341 in the test set.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">""" REDONE STUFF: </span>
<span class="sd">due to new default of sample</span>
<span class="sd">and not over-writing patients table after shuffling"""</span>
<span class="n">shuffled_patients</span> <span class="o">=</span> <span class="n">patients</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">683</span><span class="p">,</span> <span class="n">with_replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Randomly permute the rows</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">shuffled_patients</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">342</span><span class="p">))</span>
<span class="n">testset</span>  <span class="o">=</span> <span class="n">shuffled_patients</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">342</span><span class="p">,</span> <span class="mi">683</span><span class="p">))</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll train the classifier using the 342 patients in the training set, and evaluate how well it performs on the test set.  To make our lives easier, we'll write a function to evaluate a classifier on every patient in the test set:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">testattrs</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="n">numcorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="c1"># Run the classifier on the ith patient in the test set</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">testattrs</span><span class="o">.</span><span class="n">rows</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">k</span><span class="p">)</span>
        <span class="c1"># Was the classifier's prediction correct?</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">test</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">numcorrect</span> <span class="o">=</span> <span class="n">numcorrect</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">numcorrect</span> <span class="o">/</span> <span class="n">test</span><span class="o">.</span><span class="n">num_rows</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now for the grand reveal -- let's see how we did.  We'll arbitrarily use $k=5$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.9706744868035191</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>About 96% accuracy.  Not bad!  Pretty darn good for such a simple technique.</p>
<p>As a footnote, you might have noticed that Brittany Wenger did even better.  What techniques did she use? One key innovation is that she incorporated a confidence score into her results: her algorithm had a way to determine when it was not able to make a confident prediction, and for those patients, it didn't even try to predict their diagnosis.  Her algorithm was 99% accurate on the patients where it made a prediction -- so that extension seemed to help quite a bit.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">""" REDONE STUFF:</span>
<span class="sd">To separate out two main bits</span>
<span class="sd">and turn rows into arrays"""</span>

<span class="k">def</span> <span class="nf">count_zero</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">array</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">count_equal</span><span class="p">(</span><span class="n">array1</span><span class="p">,</span> <span class="n">array2</span><span class="p">):</span>
    <span class="sd">"""Takes two numerical arrays of equal length</span>
<span class="sd">    and counts the indices where the two are equal"""</span>
    <span class="k">return</span> <span class="n">count_zero</span><span class="p">(</span><span class="n">array1</span> <span class="o">-</span> <span class="n">array2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_accuracy2</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">test_attributes</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">make_array</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="c1"># Run the classifier on the ith patient in the test set</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">make_array</span><span class="p">(</span><span class="n">test_attributes</span><span class="o">.</span><span class="n">row</span><span class="p">(</span><span class="n">i</span><span class="p">)),</span> <span class="n">k</span><span class="p">))</span>   
    <span class="k">return</span> <span class="n">count_equal</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">))</span> <span class="o">/</span> <span class="n">test</span><span class="o">.</span><span class="n">num_rows</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_accuracy2</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.9706744868035191</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Important-takeaways">Important takeaways<a class="anchor-link" href="#Important-takeaways">¶</a></h3><p>Here are a few lessons we want you to learn from this.</p>
<p>First, machine learning is powerful.  If you had to try to write code to make a diagnosis without knowing about machine learning, you might spend a lot of time by trial-and-error trying to come up with some complicated set of rules that seem to work, and the result might not be very accurate.  The $k$-nearest neighbors algorithm automates the entire task for you.  And machine learning often lets them make predictions far more accurately than anything you'd come up with by trial-and-error.</p>
<p>Second, you can do it.  Yes, you.  You can use machine learning in your own work to make predictions based on data.  You now know enough to start applying these ideas to new data sets and help others make useful predictions.  The techniques are very powerful, but you don't have to have a Ph.D. in statistics to use them.</p>
<p>Third, be careful about how to evaluate accuracy.  Use a hold-out set.</p>
<p>There's lots more one can say about machine learning: how to choose attributes, how to choose $k$ or other parameters, what other classification methods are available, how to solve more complex prediction tasks, and lots more.  In this course, we've barely even scratched the surface.  If you enjoyed this material, you might enjoy continuing your studies in statistics and computer science; courses like Stats 132 and 154 and CS 188 and 189 go into a lot more depth.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Worth-Including?">Worth Including?<a class="anchor-link" href="#Worth-Including?">¶</a></h3><p>Approx 90% accuracy. All the variables are continuous, as opposed to ordinal as many of the breast cancer variables are. I'm tempted to make this one the first example, though classifying wines isn't as important as classifying cells for cancer.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wine</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">'wine.csv'</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Note there are three Class values: 1, 2, 3</span>
<span class="n">wine</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Class</th> <th>Alcohol</th> <th>Malic Acid</th> <th>Ash</th> <th>Alcalinity of Ash</th> <th>Magnesium</th> <th>Total Phenols</th> <th>Flavanoids</th> <th>Nonflavanoid phenols</th> <th>Proanthocyanins</th> <th>Color Intensity</th> <th>Hue</th> <th>OD280/OD315 of diulted wines</th> <th>Proline</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1    </td> <td>14.23  </td> <td>1.71      </td> <td>2.43</td> <td>15.6             </td> <td>127      </td> <td>2.8          </td> <td>3.06      </td> <td>0.28                </td> <td>2.29           </td> <td>5.64           </td> <td>1.04</td> <td>3.92                        </td> <td>1065   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.2   </td> <td>1.78      </td> <td>2.14</td> <td>11.2             </td> <td>100      </td> <td>2.65         </td> <td>2.76      </td> <td>0.26                </td> <td>1.28           </td> <td>4.38           </td> <td>1.05</td> <td>3.4                         </td> <td>1050   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.16  </td> <td>2.36      </td> <td>2.67</td> <td>18.6             </td> <td>101      </td> <td>2.8          </td> <td>3.24      </td> <td>0.3                 </td> <td>2.81           </td> <td>5.68           </td> <td>1.03</td> <td>3.17                        </td> <td>1185   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.37  </td> <td>1.95      </td> <td>2.5 </td> <td>16.8             </td> <td>113      </td> <td>3.85         </td> <td>3.49      </td> <td>0.24                </td> <td>2.18           </td> <td>7.8            </td> <td>0.86</td> <td>3.45                        </td> <td>1480   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.24  </td> <td>2.59      </td> <td>2.87</td> <td>21               </td> <td>118      </td> <td>2.8          </td> <td>2.69      </td> <td>0.39                </td> <td>1.82           </td> <td>4.32           </td> <td>1.04</td> <td>2.93                        </td> <td>735    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.2   </td> <td>1.76      </td> <td>2.45</td> <td>15.2             </td> <td>112      </td> <td>3.27         </td> <td>3.39      </td> <td>0.34                </td> <td>1.97           </td> <td>6.75           </td> <td>1.05</td> <td>2.85                        </td> <td>1450   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.39  </td> <td>1.87      </td> <td>2.45</td> <td>14.6             </td> <td>96       </td> <td>2.5          </td> <td>2.52      </td> <td>0.3                 </td> <td>1.98           </td> <td>5.25           </td> <td>1.02</td> <td>3.58                        </td> <td>1290   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.06  </td> <td>2.15      </td> <td>2.61</td> <td>17.6             </td> <td>121      </td> <td>2.6          </td> <td>2.51      </td> <td>0.31                </td> <td>1.25           </td> <td>5.05           </td> <td>1.06</td> <td>3.58                        </td> <td>1295   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.83  </td> <td>1.64      </td> <td>2.17</td> <td>14               </td> <td>97       </td> <td>2.8          </td> <td>2.98      </td> <td>0.29                </td> <td>1.98           </td> <td>5.2            </td> <td>1.08</td> <td>2.85                        </td> <td>1045   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.86  </td> <td>1.35      </td> <td>2.27</td> <td>16               </td> <td>98       </td> <td>2.98         </td> <td>3.15      </td> <td>0.22                </td> <td>1.85           </td> <td>7.22           </td> <td>1.01</td> <td>3.55                        </td> <td>1045   </td>
        </tr>
    </tbody>
</table>
<p>... (168 rows omitted)</p></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For converting Class to binary</span>

<span class="k">def</span> <span class="nf">is_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wine1</span> <span class="o">=</span> <span class="n">wine</span><span class="o">.</span><span class="n">with_column</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="n">wine</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">is_one</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wine1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Class</th> <th>Alcohol</th> <th>Malic Acid</th> <th>Ash</th> <th>Alcalinity of Ash</th> <th>Magnesium</th> <th>Total Phenols</th> <th>Flavanoids</th> <th>Nonflavanoid phenols</th> <th>Proanthocyanins</th> <th>Color Intensity</th> <th>Hue</th> <th>OD280/OD315 of diulted wines</th> <th>Proline</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1    </td> <td>14.23  </td> <td>1.71      </td> <td>2.43</td> <td>15.6             </td> <td>127      </td> <td>2.8          </td> <td>3.06      </td> <td>0.28                </td> <td>2.29           </td> <td>5.64           </td> <td>1.04 </td> <td>3.92                        </td> <td>1065   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.2   </td> <td>1.78      </td> <td>2.14</td> <td>11.2             </td> <td>100      </td> <td>2.65         </td> <td>2.76      </td> <td>0.26                </td> <td>1.28           </td> <td>4.38           </td> <td>1.05 </td> <td>3.4                         </td> <td>1050   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.16  </td> <td>2.36      </td> <td>2.67</td> <td>18.6             </td> <td>101      </td> <td>2.8          </td> <td>3.24      </td> <td>0.3                 </td> <td>2.81           </td> <td>5.68           </td> <td>1.03 </td> <td>3.17                        </td> <td>1185   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.37  </td> <td>1.95      </td> <td>2.5 </td> <td>16.8             </td> <td>113      </td> <td>3.85         </td> <td>3.49      </td> <td>0.24                </td> <td>2.18           </td> <td>7.8            </td> <td>0.86 </td> <td>3.45                        </td> <td>1480   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.24  </td> <td>2.59      </td> <td>2.87</td> <td>21               </td> <td>118      </td> <td>2.8          </td> <td>2.69      </td> <td>0.39                </td> <td>1.82           </td> <td>4.32           </td> <td>1.04 </td> <td>2.93                        </td> <td>735    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.2   </td> <td>1.76      </td> <td>2.45</td> <td>15.2             </td> <td>112      </td> <td>3.27         </td> <td>3.39      </td> <td>0.34                </td> <td>1.97           </td> <td>6.75           </td> <td>1.05 </td> <td>2.85                        </td> <td>1450   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.39  </td> <td>1.87      </td> <td>2.45</td> <td>14.6             </td> <td>96       </td> <td>2.5          </td> <td>2.52      </td> <td>0.3                 </td> <td>1.98           </td> <td>5.25           </td> <td>1.02 </td> <td>3.58                        </td> <td>1290   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.06  </td> <td>2.15      </td> <td>2.61</td> <td>17.6             </td> <td>121      </td> <td>2.6          </td> <td>2.51      </td> <td>0.31                </td> <td>1.25           </td> <td>5.05           </td> <td>1.06 </td> <td>3.58                        </td> <td>1295   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.83  </td> <td>1.64      </td> <td>2.17</td> <td>14               </td> <td>97       </td> <td>2.8          </td> <td>2.98      </td> <td>0.29                </td> <td>1.98           </td> <td>5.2            </td> <td>1.08 </td> <td>2.85                        </td> <td>1045   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.86  </td> <td>1.35      </td> <td>2.27</td> <td>16               </td> <td>98       </td> <td>2.98         </td> <td>3.15      </td> <td>0.22                </td> <td>1.85           </td> <td>7.22           </td> <td>1.01 </td> <td>3.55                        </td> <td>1045   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.1   </td> <td>2.16      </td> <td>2.3 </td> <td>18               </td> <td>105      </td> <td>2.95         </td> <td>3.32      </td> <td>0.22                </td> <td>2.38           </td> <td>5.75           </td> <td>1.25 </td> <td>3.17                        </td> <td>1510   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.12  </td> <td>1.48      </td> <td>2.32</td> <td>16.8             </td> <td>95       </td> <td>2.2          </td> <td>2.43      </td> <td>0.26                </td> <td>1.57           </td> <td>5              </td> <td>1.17 </td> <td>2.82                        </td> <td>1280   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.75  </td> <td>1.73      </td> <td>2.41</td> <td>16               </td> <td>89       </td> <td>2.6          </td> <td>2.76      </td> <td>0.29                </td> <td>1.81           </td> <td>5.6            </td> <td>1.15 </td> <td>2.9                         </td> <td>1320   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.75  </td> <td>1.73      </td> <td>2.39</td> <td>11.4             </td> <td>91       </td> <td>3.1          </td> <td>3.69      </td> <td>0.43                </td> <td>2.81           </td> <td>5.4            </td> <td>1.25 </td> <td>2.73                        </td> <td>1150   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.38  </td> <td>1.87      </td> <td>2.38</td> <td>12               </td> <td>102      </td> <td>3.3          </td> <td>3.64      </td> <td>0.29                </td> <td>2.96           </td> <td>7.5            </td> <td>1.2  </td> <td>3                           </td> <td>1547   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.63  </td> <td>1.81      </td> <td>2.7 </td> <td>17.2             </td> <td>112      </td> <td>2.85         </td> <td>2.91      </td> <td>0.3                 </td> <td>1.46           </td> <td>7.3            </td> <td>1.28 </td> <td>2.88                        </td> <td>1310   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.3   </td> <td>1.92      </td> <td>2.72</td> <td>20               </td> <td>120      </td> <td>2.8          </td> <td>3.14      </td> <td>0.33                </td> <td>1.97           </td> <td>6.2            </td> <td>1.07 </td> <td>2.65                        </td> <td>1280   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.83  </td> <td>1.57      </td> <td>2.62</td> <td>20               </td> <td>115      </td> <td>2.95         </td> <td>3.4       </td> <td>0.4                 </td> <td>1.72           </td> <td>6.6            </td> <td>1.13 </td> <td>2.57                        </td> <td>1130   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.19  </td> <td>1.59      </td> <td>2.48</td> <td>16.5             </td> <td>108      </td> <td>3.3          </td> <td>3.93      </td> <td>0.32                </td> <td>1.86           </td> <td>8.7            </td> <td>1.23 </td> <td>2.82                        </td> <td>1680   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.64  </td> <td>3.1       </td> <td>2.56</td> <td>15.2             </td> <td>116      </td> <td>2.7          </td> <td>3.03      </td> <td>0.17                </td> <td>1.66           </td> <td>5.1            </td> <td>0.96 </td> <td>3.36                        </td> <td>845    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.06  </td> <td>1.63      </td> <td>2.28</td> <td>16               </td> <td>126      </td> <td>3            </td> <td>3.17      </td> <td>0.24                </td> <td>2.1            </td> <td>5.65           </td> <td>1.09 </td> <td>3.71                        </td> <td>780    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>12.93  </td> <td>3.8       </td> <td>2.65</td> <td>18.6             </td> <td>102      </td> <td>2.41         </td> <td>2.41      </td> <td>0.25                </td> <td>1.98           </td> <td>4.5            </td> <td>1.03 </td> <td>3.52                        </td> <td>770    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.71  </td> <td>1.86      </td> <td>2.36</td> <td>16.6             </td> <td>101      </td> <td>2.61         </td> <td>2.88      </td> <td>0.27                </td> <td>1.69           </td> <td>3.8            </td> <td>1.11 </td> <td>4                           </td> <td>1035   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>12.85  </td> <td>1.6       </td> <td>2.52</td> <td>17.8             </td> <td>95       </td> <td>2.48         </td> <td>2.37      </td> <td>0.26                </td> <td>1.46           </td> <td>3.93           </td> <td>1.09 </td> <td>3.63                        </td> <td>1015   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.5   </td> <td>1.81      </td> <td>2.61</td> <td>20               </td> <td>96       </td> <td>2.53         </td> <td>2.61      </td> <td>0.28                </td> <td>1.66           </td> <td>3.52           </td> <td>1.12 </td> <td>3.82                        </td> <td>845    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.05  </td> <td>2.05      </td> <td>3.22</td> <td>25               </td> <td>124      </td> <td>2.63         </td> <td>2.68      </td> <td>0.47                </td> <td>1.92           </td> <td>3.58           </td> <td>1.13 </td> <td>3.2                         </td> <td>830    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.39  </td> <td>1.77      </td> <td>2.62</td> <td>16.1             </td> <td>93       </td> <td>2.85         </td> <td>2.94      </td> <td>0.34                </td> <td>1.45           </td> <td>4.8            </td> <td>0.92 </td> <td>3.22                        </td> <td>1195   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.3   </td> <td>1.72      </td> <td>2.14</td> <td>17               </td> <td>94       </td> <td>2.4          </td> <td>2.19      </td> <td>0.27                </td> <td>1.35           </td> <td>3.95           </td> <td>1.02 </td> <td>2.77                        </td> <td>1285   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.87  </td> <td>1.9       </td> <td>2.8 </td> <td>19.4             </td> <td>107      </td> <td>2.95         </td> <td>2.97      </td> <td>0.37                </td> <td>1.76           </td> <td>4.5            </td> <td>1.25 </td> <td>3.4                         </td> <td>915    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.02  </td> <td>1.68      </td> <td>2.21</td> <td>16               </td> <td>96       </td> <td>2.65         </td> <td>2.33      </td> <td>0.26                </td> <td>1.98           </td> <td>4.7            </td> <td>1.04 </td> <td>3.59                        </td> <td>1035   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.73  </td> <td>1.5       </td> <td>2.7 </td> <td>22.5             </td> <td>101      </td> <td>3            </td> <td>3.25      </td> <td>0.29                </td> <td>2.38           </td> <td>5.7            </td> <td>1.19 </td> <td>2.71                        </td> <td>1285   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.58  </td> <td>1.66      </td> <td>2.36</td> <td>19.1             </td> <td>106      </td> <td>2.86         </td> <td>3.19      </td> <td>0.22                </td> <td>1.95           </td> <td>6.9            </td> <td>1.09 </td> <td>2.88                        </td> <td>1515   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.68  </td> <td>1.83      </td> <td>2.36</td> <td>17.2             </td> <td>104      </td> <td>2.42         </td> <td>2.69      </td> <td>0.42                </td> <td>1.97           </td> <td>3.84           </td> <td>1.23 </td> <td>2.87                        </td> <td>990    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.76  </td> <td>1.53      </td> <td>2.7 </td> <td>19.5             </td> <td>132      </td> <td>2.95         </td> <td>2.74      </td> <td>0.5                 </td> <td>1.35           </td> <td>5.4            </td> <td>1.25 </td> <td>3                           </td> <td>1235   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.51  </td> <td>1.8       </td> <td>2.65</td> <td>19               </td> <td>110      </td> <td>2.35         </td> <td>2.53      </td> <td>0.29                </td> <td>1.54           </td> <td>4.2            </td> <td>1.1  </td> <td>2.87                        </td> <td>1095   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.48  </td> <td>1.81      </td> <td>2.41</td> <td>20.5             </td> <td>100      </td> <td>2.7          </td> <td>2.98      </td> <td>0.26                </td> <td>1.86           </td> <td>5.1            </td> <td>1.04 </td> <td>3.47                        </td> <td>920    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.28  </td> <td>1.64      </td> <td>2.84</td> <td>15.5             </td> <td>110      </td> <td>2.6          </td> <td>2.68      </td> <td>0.34                </td> <td>1.36           </td> <td>4.6            </td> <td>1.09 </td> <td>2.78                        </td> <td>880    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.05  </td> <td>1.65      </td> <td>2.55</td> <td>18               </td> <td>98       </td> <td>2.45         </td> <td>2.43      </td> <td>0.29                </td> <td>1.44           </td> <td>4.25           </td> <td>1.12 </td> <td>2.51                        </td> <td>1105   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.07  </td> <td>1.5       </td> <td>2.1 </td> <td>15.5             </td> <td>98       </td> <td>2.4          </td> <td>2.64      </td> <td>0.28                </td> <td>1.37           </td> <td>3.7            </td> <td>1.18 </td> <td>2.69                        </td> <td>1020   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.22  </td> <td>3.99      </td> <td>2.51</td> <td>13.2             </td> <td>128      </td> <td>3            </td> <td>3.04      </td> <td>0.2                 </td> <td>2.08           </td> <td>5.1            </td> <td>0.89 </td> <td>3.53                        </td> <td>760    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.56  </td> <td>1.71      </td> <td>2.31</td> <td>16.2             </td> <td>117      </td> <td>3.15         </td> <td>3.29      </td> <td>0.34                </td> <td>2.34           </td> <td>6.13           </td> <td>0.95 </td> <td>3.38                        </td> <td>795    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.41  </td> <td>3.84      </td> <td>2.12</td> <td>18.8             </td> <td>90       </td> <td>2.45         </td> <td>2.68      </td> <td>0.27                </td> <td>1.48           </td> <td>4.28           </td> <td>0.91 </td> <td>3                           </td> <td>1035   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.88  </td> <td>1.89      </td> <td>2.59</td> <td>15               </td> <td>101      </td> <td>3.25         </td> <td>3.56      </td> <td>0.17                </td> <td>1.7            </td> <td>5.43           </td> <td>0.88 </td> <td>3.56                        </td> <td>1095   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.24  </td> <td>3.98      </td> <td>2.29</td> <td>17.5             </td> <td>103      </td> <td>2.64         </td> <td>2.63      </td> <td>0.32                </td> <td>1.66           </td> <td>4.36           </td> <td>0.82 </td> <td>3                           </td> <td>680    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.05  </td> <td>1.77      </td> <td>2.1 </td> <td>17               </td> <td>107      </td> <td>3            </td> <td>3         </td> <td>0.28                </td> <td>2.03           </td> <td>5.04           </td> <td>0.88 </td> <td>3.35                        </td> <td>885    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.21  </td> <td>4.04      </td> <td>2.44</td> <td>18.9             </td> <td>111      </td> <td>2.85         </td> <td>2.65      </td> <td>0.3                 </td> <td>1.25           </td> <td>5.24           </td> <td>0.87 </td> <td>3.33                        </td> <td>1080   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.38  </td> <td>3.59      </td> <td>2.28</td> <td>16               </td> <td>102      </td> <td>3.25         </td> <td>3.17      </td> <td>0.27                </td> <td>2.19           </td> <td>4.9            </td> <td>1.04 </td> <td>3.44                        </td> <td>1065   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.9   </td> <td>1.68      </td> <td>2.12</td> <td>16               </td> <td>101      </td> <td>3.1          </td> <td>3.39      </td> <td>0.21                </td> <td>2.14           </td> <td>6.1            </td> <td>0.91 </td> <td>3.33                        </td> <td>985    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.1   </td> <td>2.02      </td> <td>2.4 </td> <td>18.8             </td> <td>103      </td> <td>2.75         </td> <td>2.92      </td> <td>0.32                </td> <td>2.38           </td> <td>6.2            </td> <td>1.07 </td> <td>2.75                        </td> <td>1060   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.94  </td> <td>1.73      </td> <td>2.27</td> <td>17.4             </td> <td>108      </td> <td>2.88         </td> <td>3.54      </td> <td>0.32                </td> <td>2.08           </td> <td>8.9            </td> <td>1.12 </td> <td>3.1                         </td> <td>1260   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.05  </td> <td>1.73      </td> <td>2.04</td> <td>12.4             </td> <td>92       </td> <td>2.72         </td> <td>3.27      </td> <td>0.17                </td> <td>2.91           </td> <td>7.2            </td> <td>1.12 </td> <td>2.91                        </td> <td>1150   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.83  </td> <td>1.65      </td> <td>2.6 </td> <td>17.2             </td> <td>94       </td> <td>2.45         </td> <td>2.99      </td> <td>0.22                </td> <td>2.29           </td> <td>5.6            </td> <td>1.24 </td> <td>3.37                        </td> <td>1265   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.82  </td> <td>1.75      </td> <td>2.42</td> <td>14               </td> <td>111      </td> <td>3.88         </td> <td>3.74      </td> <td>0.32                </td> <td>1.87           </td> <td>7.05           </td> <td>1.01 </td> <td>3.26                        </td> <td>1190   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.77  </td> <td>1.9       </td> <td>2.68</td> <td>17.1             </td> <td>115      </td> <td>3            </td> <td>2.79      </td> <td>0.39                </td> <td>1.68           </td> <td>6.3            </td> <td>1.13 </td> <td>2.93                        </td> <td>1375   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.74  </td> <td>1.67      </td> <td>2.25</td> <td>16.4             </td> <td>118      </td> <td>2.6          </td> <td>2.9       </td> <td>0.21                </td> <td>1.62           </td> <td>5.85           </td> <td>0.92 </td> <td>3.2                         </td> <td>1060   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.56  </td> <td>1.73      </td> <td>2.46</td> <td>20.5             </td> <td>116      </td> <td>2.96         </td> <td>2.78      </td> <td>0.2                 </td> <td>2.45           </td> <td>6.25           </td> <td>0.98 </td> <td>3.03                        </td> <td>1120   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>14.22  </td> <td>1.7       </td> <td>2.3 </td> <td>16.3             </td> <td>118      </td> <td>3.2          </td> <td>3         </td> <td>0.26                </td> <td>2.03           </td> <td>6.38           </td> <td>0.94 </td> <td>3.31                        </td> <td>970    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.29  </td> <td>1.97      </td> <td>2.68</td> <td>16.8             </td> <td>102      </td> <td>3            </td> <td>3.23      </td> <td>0.31                </td> <td>1.66           </td> <td>6              </td> <td>1.07 </td> <td>2.84                        </td> <td>1270   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>1    </td> <td>13.72  </td> <td>1.43      </td> <td>2.5 </td> <td>16.7             </td> <td>108      </td> <td>3.4          </td> <td>3.67      </td> <td>0.19                </td> <td>2.04           </td> <td>6.8            </td> <td>0.89 </td> <td>2.87                        </td> <td>1285   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>0.94      </td> <td>1.36</td> <td>10.6             </td> <td>88       </td> <td>1.98         </td> <td>0.57      </td> <td>0.28                </td> <td>0.42           </td> <td>1.95           </td> <td>1.05 </td> <td>1.82                        </td> <td>520    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.33  </td> <td>1.1       </td> <td>2.28</td> <td>16               </td> <td>101      </td> <td>2.05         </td> <td>1.09      </td> <td>0.63                </td> <td>0.41           </td> <td>3.27           </td> <td>1.25 </td> <td>1.67                        </td> <td>680    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.64  </td> <td>1.36      </td> <td>2.02</td> <td>16.8             </td> <td>100      </td> <td>2.02         </td> <td>1.41      </td> <td>0.53                </td> <td>0.62           </td> <td>5.75           </td> <td>0.98 </td> <td>1.59                        </td> <td>450    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.67  </td> <td>1.25      </td> <td>1.92</td> <td>18               </td> <td>94       </td> <td>2.1          </td> <td>1.79      </td> <td>0.32                </td> <td>0.73           </td> <td>3.8            </td> <td>1.23 </td> <td>2.46                        </td> <td>630    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>1.13      </td> <td>2.16</td> <td>19               </td> <td>87       </td> <td>3.5          </td> <td>3.1       </td> <td>0.19                </td> <td>1.87           </td> <td>4.45           </td> <td>1.22 </td> <td>2.87                        </td> <td>420    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.17  </td> <td>1.45      </td> <td>2.53</td> <td>19               </td> <td>104      </td> <td>1.89         </td> <td>1.75      </td> <td>0.45                </td> <td>1.03           </td> <td>2.95           </td> <td>1.45 </td> <td>2.23                        </td> <td>355    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>1.21      </td> <td>2.56</td> <td>18.1             </td> <td>98       </td> <td>2.42         </td> <td>2.65      </td> <td>0.37                </td> <td>2.08           </td> <td>4.6            </td> <td>1.19 </td> <td>2.3                         </td> <td>678    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.11  </td> <td>1.01      </td> <td>1.7 </td> <td>15               </td> <td>78       </td> <td>2.98         </td> <td>3.18      </td> <td>0.26                </td> <td>2.28           </td> <td>5.3            </td> <td>1.12 </td> <td>3.18                        </td> <td>502    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>1.17      </td> <td>1.92</td> <td>19.6             </td> <td>78       </td> <td>2.11         </td> <td>2         </td> <td>0.27                </td> <td>1.04           </td> <td>4.68           </td> <td>1.12 </td> <td>3.48                        </td> <td>510    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.34  </td> <td>0.94      </td> <td>2.36</td> <td>17               </td> <td>110      </td> <td>2.53         </td> <td>1.3       </td> <td>0.55                </td> <td>0.42           </td> <td>3.17           </td> <td>1.02 </td> <td>1.93                        </td> <td>750    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.21  </td> <td>1.19      </td> <td>1.75</td> <td>16.8             </td> <td>151      </td> <td>1.85         </td> <td>1.28      </td> <td>0.14                </td> <td>2.5            </td> <td>2.85           </td> <td>1.28 </td> <td>3.07                        </td> <td>718    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.29  </td> <td>1.61      </td> <td>2.21</td> <td>20.4             </td> <td>103      </td> <td>1.1          </td> <td>1.02      </td> <td>0.37                </td> <td>1.46           </td> <td>3.05           </td> <td>0.906</td> <td>1.82                        </td> <td>870    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.86  </td> <td>1.51      </td> <td>2.67</td> <td>25               </td> <td>86       </td> <td>2.95         </td> <td>2.86      </td> <td>0.21                </td> <td>1.87           </td> <td>3.38           </td> <td>1.36 </td> <td>3.16                        </td> <td>410    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.49  </td> <td>1.66      </td> <td>2.24</td> <td>24               </td> <td>87       </td> <td>1.88         </td> <td>1.84      </td> <td>0.27                </td> <td>1.03           </td> <td>3.74           </td> <td>0.98 </td> <td>2.78                        </td> <td>472    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.99  </td> <td>1.67      </td> <td>2.6 </td> <td>30               </td> <td>139      </td> <td>3.3          </td> <td>2.89      </td> <td>0.21                </td> <td>1.96           </td> <td>3.35           </td> <td>1.31 </td> <td>3.5                         </td> <td>985    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.96  </td> <td>1.09      </td> <td>2.3 </td> <td>21               </td> <td>101      </td> <td>3.38         </td> <td>2.14      </td> <td>0.13                </td> <td>1.65           </td> <td>3.21           </td> <td>0.99 </td> <td>3.13                        </td> <td>886    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.66  </td> <td>1.88      </td> <td>1.92</td> <td>16               </td> <td>97       </td> <td>1.61         </td> <td>1.57      </td> <td>0.34                </td> <td>1.15           </td> <td>3.8            </td> <td>1.23 </td> <td>2.14                        </td> <td>428    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.03  </td> <td>0.9       </td> <td>1.71</td> <td>16               </td> <td>86       </td> <td>1.95         </td> <td>2.03      </td> <td>0.24                </td> <td>1.46           </td> <td>4.6            </td> <td>1.19 </td> <td>2.48                        </td> <td>392    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.84  </td> <td>2.89      </td> <td>2.23</td> <td>18               </td> <td>112      </td> <td>1.72         </td> <td>1.32      </td> <td>0.43                </td> <td>0.95           </td> <td>2.65           </td> <td>0.96 </td> <td>2.52                        </td> <td>500    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.33  </td> <td>0.99      </td> <td>1.95</td> <td>14.8             </td> <td>136      </td> <td>1.9          </td> <td>1.85      </td> <td>0.35                </td> <td>2.76           </td> <td>3.4            </td> <td>1.06 </td> <td>2.31                        </td> <td>750    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.7   </td> <td>3.87      </td> <td>2.4 </td> <td>23               </td> <td>101      </td> <td>2.83         </td> <td>2.55      </td> <td>0.43                </td> <td>1.95           </td> <td>2.57           </td> <td>1.19 </td> <td>3.13                        </td> <td>463    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12     </td> <td>0.92      </td> <td>2   </td> <td>19               </td> <td>86       </td> <td>2.42         </td> <td>2.26      </td> <td>0.3                 </td> <td>1.43           </td> <td>2.5            </td> <td>1.38 </td> <td>3.12                        </td> <td>278    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.72  </td> <td>1.81      </td> <td>2.2 </td> <td>18.8             </td> <td>86       </td> <td>2.2          </td> <td>2.53      </td> <td>0.26                </td> <td>1.77           </td> <td>3.9            </td> <td>1.16 </td> <td>3.14                        </td> <td>714    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.08  </td> <td>1.13      </td> <td>2.51</td> <td>24               </td> <td>78       </td> <td>2            </td> <td>1.58      </td> <td>0.4                 </td> <td>1.4            </td> <td>2.2            </td> <td>1.31 </td> <td>2.72                        </td> <td>630    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.05  </td> <td>3.86      </td> <td>2.32</td> <td>22.5             </td> <td>85       </td> <td>1.65         </td> <td>1.59      </td> <td>0.61                </td> <td>1.62           </td> <td>4.8            </td> <td>0.84 </td> <td>2.01                        </td> <td>515    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.84  </td> <td>0.89      </td> <td>2.58</td> <td>18               </td> <td>94       </td> <td>2.2          </td> <td>2.21      </td> <td>0.22                </td> <td>2.35           </td> <td>3.05           </td> <td>0.79 </td> <td>3.08                        </td> <td>520    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.67  </td> <td>0.98      </td> <td>2.24</td> <td>18               </td> <td>99       </td> <td>2.2          </td> <td>1.94      </td> <td>0.3                 </td> <td>1.46           </td> <td>2.62           </td> <td>1.23 </td> <td>3.16                        </td> <td>450    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.16  </td> <td>1.61      </td> <td>2.31</td> <td>22.8             </td> <td>90       </td> <td>1.78         </td> <td>1.69      </td> <td>0.43                </td> <td>1.56           </td> <td>2.45           </td> <td>1.33 </td> <td>2.26                        </td> <td>495    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.65  </td> <td>1.67      </td> <td>2.62</td> <td>26               </td> <td>88       </td> <td>1.92         </td> <td>1.61      </td> <td>0.4                 </td> <td>1.34           </td> <td>2.6            </td> <td>1.36 </td> <td>3.21                        </td> <td>562    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.64  </td> <td>2.06      </td> <td>2.46</td> <td>21.6             </td> <td>84       </td> <td>1.95         </td> <td>1.69      </td> <td>0.48                </td> <td>1.35           </td> <td>2.8            </td> <td>1    </td> <td>2.75                        </td> <td>680    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.08  </td> <td>1.33      </td> <td>2.3 </td> <td>23.6             </td> <td>70       </td> <td>2.2          </td> <td>1.59      </td> <td>0.42                </td> <td>1.38           </td> <td>1.74           </td> <td>1.07 </td> <td>3.21                        </td> <td>625    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.08  </td> <td>1.83      </td> <td>2.32</td> <td>18.5             </td> <td>81       </td> <td>1.6          </td> <td>1.5       </td> <td>0.52                </td> <td>1.64           </td> <td>2.4            </td> <td>1.08 </td> <td>2.27                        </td> <td>480    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12     </td> <td>1.51      </td> <td>2.42</td> <td>22               </td> <td>86       </td> <td>1.45         </td> <td>1.25      </td> <td>0.5                 </td> <td>1.63           </td> <td>3.6            </td> <td>1.05 </td> <td>2.65                        </td> <td>450    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.69  </td> <td>1.53      </td> <td>2.26</td> <td>20.7             </td> <td>80       </td> <td>1.38         </td> <td>1.46      </td> <td>0.58                </td> <td>1.62           </td> <td>3.05           </td> <td>0.96 </td> <td>2.06                        </td> <td>495    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.29  </td> <td>2.83      </td> <td>2.22</td> <td>18               </td> <td>88       </td> <td>2.45         </td> <td>2.25      </td> <td>0.25                </td> <td>1.99           </td> <td>2.15           </td> <td>1.15 </td> <td>3.3                         </td> <td>290    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.62  </td> <td>1.99      </td> <td>2.28</td> <td>18               </td> <td>98       </td> <td>3.02         </td> <td>2.26      </td> <td>0.17                </td> <td>1.35           </td> <td>3.25           </td> <td>1.16 </td> <td>2.96                        </td> <td>345    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.47  </td> <td>1.52      </td> <td>2.2 </td> <td>19               </td> <td>162      </td> <td>2.5          </td> <td>2.27      </td> <td>0.32                </td> <td>3.28           </td> <td>2.6            </td> <td>1.16 </td> <td>2.63                        </td> <td>937    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.81  </td> <td>2.12      </td> <td>2.74</td> <td>21.5             </td> <td>134      </td> <td>1.6          </td> <td>0.99      </td> <td>0.14                </td> <td>1.56           </td> <td>2.5            </td> <td>0.95 </td> <td>2.26                        </td> <td>625    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.29  </td> <td>1.41      </td> <td>1.98</td> <td>16               </td> <td>85       </td> <td>2.55         </td> <td>2.5       </td> <td>0.29                </td> <td>1.77           </td> <td>2.9            </td> <td>1.23 </td> <td>2.74                        </td> <td>428    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>1.07      </td> <td>2.1 </td> <td>18.5             </td> <td>88       </td> <td>3.52         </td> <td>3.75      </td> <td>0.24                </td> <td>1.95           </td> <td>4.5            </td> <td>1.04 </td> <td>2.77                        </td> <td>660    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.29  </td> <td>3.17      </td> <td>2.21</td> <td>18               </td> <td>88       </td> <td>2.85         </td> <td>2.99      </td> <td>0.45                </td> <td>2.81           </td> <td>2.3            </td> <td>1.42 </td> <td>2.83                        </td> <td>406    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.08  </td> <td>2.08      </td> <td>1.7 </td> <td>17.5             </td> <td>97       </td> <td>2.23         </td> <td>2.17      </td> <td>0.26                </td> <td>1.4            </td> <td>3.3            </td> <td>1.27 </td> <td>2.96                        </td> <td>710    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.6   </td> <td>1.34      </td> <td>1.9 </td> <td>18.5             </td> <td>88       </td> <td>1.45         </td> <td>1.36      </td> <td>0.29                </td> <td>1.35           </td> <td>2.45           </td> <td>1.04 </td> <td>2.77                        </td> <td>562    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.34  </td> <td>2.45      </td> <td>2.46</td> <td>21               </td> <td>98       </td> <td>2.56         </td> <td>2.11      </td> <td>0.34                </td> <td>1.31           </td> <td>2.8            </td> <td>0.8  </td> <td>3.38                        </td> <td>438    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.82  </td> <td>1.72      </td> <td>1.88</td> <td>19.5             </td> <td>86       </td> <td>2.5          </td> <td>1.64      </td> <td>0.37                </td> <td>1.42           </td> <td>2.06           </td> <td>0.94 </td> <td>2.44                        </td> <td>415    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.51  </td> <td>1.73      </td> <td>1.98</td> <td>20.5             </td> <td>85       </td> <td>2.2          </td> <td>1.92      </td> <td>0.32                </td> <td>1.48           </td> <td>2.94           </td> <td>1.04 </td> <td>3.57                        </td> <td>672    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.42  </td> <td>2.55      </td> <td>2.27</td> <td>22               </td> <td>90       </td> <td>1.68         </td> <td>1.84      </td> <td>0.66                </td> <td>1.42           </td> <td>2.7            </td> <td>0.86 </td> <td>3.3                         </td> <td>315    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.25  </td> <td>1.73      </td> <td>2.12</td> <td>19               </td> <td>80       </td> <td>1.65         </td> <td>2.03      </td> <td>0.37                </td> <td>1.63           </td> <td>3.4            </td> <td>1    </td> <td>3.17                        </td> <td>510    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.72  </td> <td>1.75      </td> <td>2.28</td> <td>22.5             </td> <td>84       </td> <td>1.38         </td> <td>1.76      </td> <td>0.48                </td> <td>1.63           </td> <td>3.3            </td> <td>0.88 </td> <td>2.42                        </td> <td>488    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.22  </td> <td>1.29      </td> <td>1.94</td> <td>19               </td> <td>92       </td> <td>2.36         </td> <td>2.04      </td> <td>0.39                </td> <td>2.08           </td> <td>2.7            </td> <td>0.86 </td> <td>3.02                        </td> <td>312    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.61  </td> <td>1.35      </td> <td>2.7 </td> <td>20               </td> <td>94       </td> <td>2.74         </td> <td>2.92      </td> <td>0.29                </td> <td>2.49           </td> <td>2.65           </td> <td>0.96 </td> <td>3.26                        </td> <td>680    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.46  </td> <td>3.74      </td> <td>1.82</td> <td>19.5             </td> <td>107      </td> <td>3.18         </td> <td>2.58      </td> <td>0.24                </td> <td>3.58           </td> <td>2.9            </td> <td>0.75 </td> <td>2.81                        </td> <td>562    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.52  </td> <td>2.43      </td> <td>2.17</td> <td>21               </td> <td>88       </td> <td>2.55         </td> <td>2.27      </td> <td>0.26                </td> <td>1.22           </td> <td>2              </td> <td>0.9  </td> <td>2.78                        </td> <td>325    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.76  </td> <td>2.68      </td> <td>2.92</td> <td>20               </td> <td>103      </td> <td>1.75         </td> <td>2.03      </td> <td>0.6                 </td> <td>1.05           </td> <td>3.8            </td> <td>1.23 </td> <td>2.5                         </td> <td>607    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.41  </td> <td>0.74      </td> <td>2.5 </td> <td>21               </td> <td>88       </td> <td>2.48         </td> <td>2.01      </td> <td>0.42                </td> <td>1.44           </td> <td>3.08           </td> <td>1.1  </td> <td>2.31                        </td> <td>434    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.08  </td> <td>1.39      </td> <td>2.5 </td> <td>22.5             </td> <td>84       </td> <td>2.56         </td> <td>2.29      </td> <td>0.43                </td> <td>1.04           </td> <td>2.9            </td> <td>0.93 </td> <td>3.19                        </td> <td>385    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.03  </td> <td>1.51      </td> <td>2.2 </td> <td>21.5             </td> <td>85       </td> <td>2.46         </td> <td>2.17      </td> <td>0.52                </td> <td>2.01           </td> <td>1.9            </td> <td>1.71 </td> <td>2.87                        </td> <td>407    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.82  </td> <td>1.47      </td> <td>1.99</td> <td>20.8             </td> <td>86       </td> <td>1.98         </td> <td>1.6       </td> <td>0.3                 </td> <td>1.53           </td> <td>1.95           </td> <td>0.95 </td> <td>3.33                        </td> <td>495    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.42  </td> <td>1.61      </td> <td>2.19</td> <td>22.5             </td> <td>108      </td> <td>2            </td> <td>2.09      </td> <td>0.34                </td> <td>1.61           </td> <td>2.06           </td> <td>1.06 </td> <td>2.96                        </td> <td>345    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.77  </td> <td>3.43      </td> <td>1.98</td> <td>16               </td> <td>80       </td> <td>1.63         </td> <td>1.25      </td> <td>0.43                </td> <td>0.83           </td> <td>3.4            </td> <td>0.7  </td> <td>2.12                        </td> <td>372    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12     </td> <td>3.43      </td> <td>2   </td> <td>19               </td> <td>87       </td> <td>2            </td> <td>1.64      </td> <td>0.37                </td> <td>1.87           </td> <td>1.28           </td> <td>0.93 </td> <td>3.05                        </td> <td>564    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.45  </td> <td>2.4       </td> <td>2.42</td> <td>20               </td> <td>96       </td> <td>2.9          </td> <td>2.79      </td> <td>0.32                </td> <td>1.83           </td> <td>3.25           </td> <td>0.8  </td> <td>3.39                        </td> <td>625    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.56  </td> <td>2.05      </td> <td>3.23</td> <td>28.5             </td> <td>119      </td> <td>3.18         </td> <td>5.08      </td> <td>0.47                </td> <td>1.87           </td> <td>6              </td> <td>0.93 </td> <td>3.69                        </td> <td>465    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.42  </td> <td>4.43      </td> <td>2.73</td> <td>26.5             </td> <td>102      </td> <td>2.2          </td> <td>2.13      </td> <td>0.43                </td> <td>1.71           </td> <td>2.08           </td> <td>0.92 </td> <td>3.12                        </td> <td>365    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.05  </td> <td>5.8       </td> <td>2.13</td> <td>21.5             </td> <td>86       </td> <td>2.62         </td> <td>2.65      </td> <td>0.3                 </td> <td>2.01           </td> <td>2.6            </td> <td>0.73 </td> <td>3.1                         </td> <td>380    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.87  </td> <td>4.31      </td> <td>2.39</td> <td>21               </td> <td>82       </td> <td>2.86         </td> <td>3.03      </td> <td>0.21                </td> <td>2.91           </td> <td>2.8            </td> <td>0.75 </td> <td>3.64                        </td> <td>380    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.07  </td> <td>2.16      </td> <td>2.17</td> <td>21               </td> <td>85       </td> <td>2.6          </td> <td>2.65      </td> <td>0.37                </td> <td>1.35           </td> <td>2.76           </td> <td>0.86 </td> <td>3.28                        </td> <td>378    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.43  </td> <td>1.53      </td> <td>2.29</td> <td>21.5             </td> <td>86       </td> <td>2.74         </td> <td>3.15      </td> <td>0.39                </td> <td>1.77           </td> <td>3.94           </td> <td>0.69 </td> <td>2.84                        </td> <td>352    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>11.79  </td> <td>2.13      </td> <td>2.78</td> <td>28.5             </td> <td>92       </td> <td>2.13         </td> <td>2.24      </td> <td>0.58                </td> <td>1.76           </td> <td>3              </td> <td>0.97 </td> <td>2.44                        </td> <td>466    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.37  </td> <td>1.63      </td> <td>2.3 </td> <td>24.5             </td> <td>88       </td> <td>2.22         </td> <td>2.45      </td> <td>0.4                 </td> <td>1.9            </td> <td>2.12           </td> <td>0.89 </td> <td>2.78                        </td> <td>342    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.04  </td> <td>4.3       </td> <td>2.38</td> <td>22               </td> <td>80       </td> <td>2.1          </td> <td>1.75      </td> <td>0.42                </td> <td>1.35           </td> <td>2.6            </td> <td>0.79 </td> <td>2.57                        </td> <td>580    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.86  </td> <td>1.35      </td> <td>2.32</td> <td>18               </td> <td>122      </td> <td>1.51         </td> <td>1.25      </td> <td>0.21                </td> <td>0.94           </td> <td>4.1            </td> <td>0.76 </td> <td>1.29                        </td> <td>630    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.88  </td> <td>2.99      </td> <td>2.4 </td> <td>20               </td> <td>104      </td> <td>1.3          </td> <td>1.22      </td> <td>0.24                </td> <td>0.83           </td> <td>5.4            </td> <td>0.74 </td> <td>1.42                        </td> <td>530    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.81  </td> <td>2.31      </td> <td>2.4 </td> <td>24               </td> <td>98       </td> <td>1.15         </td> <td>1.09      </td> <td>0.27                </td> <td>0.83           </td> <td>5.7            </td> <td>0.66 </td> <td>1.36                        </td> <td>560    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.7   </td> <td>3.55      </td> <td>2.36</td> <td>21.5             </td> <td>106      </td> <td>1.7          </td> <td>1.2       </td> <td>0.17                </td> <td>0.84           </td> <td>5              </td> <td>0.78 </td> <td>1.29                        </td> <td>600    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.51  </td> <td>1.24      </td> <td>2.25</td> <td>17.5             </td> <td>85       </td> <td>2            </td> <td>0.58      </td> <td>0.6                 </td> <td>1.25           </td> <td>5.45           </td> <td>0.75 </td> <td>1.51                        </td> <td>650    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.6   </td> <td>2.46      </td> <td>2.2 </td> <td>18.5             </td> <td>94       </td> <td>1.62         </td> <td>0.66      </td> <td>0.63                </td> <td>0.94           </td> <td>7.1            </td> <td>0.73 </td> <td>1.58                        </td> <td>695    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.25  </td> <td>4.72      </td> <td>2.54</td> <td>21               </td> <td>89       </td> <td>1.38         </td> <td>0.47      </td> <td>0.53                </td> <td>0.8            </td> <td>3.85           </td> <td>0.75 </td> <td>1.27                        </td> <td>720    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.53  </td> <td>5.51      </td> <td>2.64</td> <td>25               </td> <td>96       </td> <td>1.79         </td> <td>0.6       </td> <td>0.63                </td> <td>1.1            </td> <td>5              </td> <td>0.82 </td> <td>1.69                        </td> <td>515    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.49  </td> <td>3.59      </td> <td>2.19</td> <td>19.5             </td> <td>88       </td> <td>1.62         </td> <td>0.48      </td> <td>0.58                </td> <td>0.88           </td> <td>5.7            </td> <td>0.81 </td> <td>1.82                        </td> <td>580    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.84  </td> <td>2.96      </td> <td>2.61</td> <td>24               </td> <td>101      </td> <td>2.32         </td> <td>0.6       </td> <td>0.53                </td> <td>0.81           </td> <td>4.92           </td> <td>0.89 </td> <td>2.15                        </td> <td>590    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.93  </td> <td>2.81      </td> <td>2.7 </td> <td>21               </td> <td>96       </td> <td>1.54         </td> <td>0.5       </td> <td>0.53                </td> <td>0.75           </td> <td>4.6            </td> <td>0.77 </td> <td>2.31                        </td> <td>600    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.36  </td> <td>2.56      </td> <td>2.35</td> <td>20               </td> <td>89       </td> <td>1.4          </td> <td>0.5       </td> <td>0.37                </td> <td>0.64           </td> <td>5.6            </td> <td>0.7  </td> <td>2.47                        </td> <td>780    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.52  </td> <td>3.17      </td> <td>2.72</td> <td>23.5             </td> <td>97       </td> <td>1.55         </td> <td>0.52      </td> <td>0.5                 </td> <td>0.55           </td> <td>4.35           </td> <td>0.89 </td> <td>2.06                        </td> <td>520    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.62  </td> <td>4.95      </td> <td>2.35</td> <td>20               </td> <td>92       </td> <td>2            </td> <td>0.8       </td> <td>0.47                </td> <td>1.02           </td> <td>4.4            </td> <td>0.91 </td> <td>2.05                        </td> <td>550    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.25  </td> <td>3.88      </td> <td>2.2 </td> <td>18.5             </td> <td>112      </td> <td>1.38         </td> <td>0.78      </td> <td>0.29                </td> <td>1.14           </td> <td>8.21           </td> <td>0.65 </td> <td>2                           </td> <td>855    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.16  </td> <td>3.57      </td> <td>2.15</td> <td>21               </td> <td>102      </td> <td>1.5          </td> <td>0.55      </td> <td>0.43                </td> <td>1.3            </td> <td>4              </td> <td>0.6  </td> <td>1.68                        </td> <td>830    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.88  </td> <td>5.04      </td> <td>2.23</td> <td>20               </td> <td>80       </td> <td>0.98         </td> <td>0.34      </td> <td>0.4                 </td> <td>0.68           </td> <td>4.9            </td> <td>0.58 </td> <td>1.33                        </td> <td>415    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.87  </td> <td>4.61      </td> <td>2.48</td> <td>21.5             </td> <td>86       </td> <td>1.7          </td> <td>0.65      </td> <td>0.47                </td> <td>0.86           </td> <td>7.65           </td> <td>0.54 </td> <td>1.86                        </td> <td>625    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.32  </td> <td>3.24      </td> <td>2.38</td> <td>21.5             </td> <td>92       </td> <td>1.93         </td> <td>0.76      </td> <td>0.45                </td> <td>1.25           </td> <td>8.42           </td> <td>0.55 </td> <td>1.62                        </td> <td>650    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.08  </td> <td>3.9       </td> <td>2.36</td> <td>21.5             </td> <td>113      </td> <td>1.41         </td> <td>1.39      </td> <td>0.34                </td> <td>1.14           </td> <td>9.4            </td> <td>0.57 </td> <td>1.33                        </td> <td>550    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.5   </td> <td>3.12      </td> <td>2.62</td> <td>24               </td> <td>123      </td> <td>1.4          </td> <td>1.57      </td> <td>0.22                </td> <td>1.25           </td> <td>8.6            </td> <td>0.59 </td> <td>1.3                         </td> <td>500    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.79  </td> <td>2.67      </td> <td>2.48</td> <td>22               </td> <td>112      </td> <td>1.48         </td> <td>1.36      </td> <td>0.24                </td> <td>1.26           </td> <td>10.8           </td> <td>0.48 </td> <td>1.47                        </td> <td>480    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.11  </td> <td>1.9       </td> <td>2.75</td> <td>25.5             </td> <td>116      </td> <td>2.2          </td> <td>1.28      </td> <td>0.26                </td> <td>1.56           </td> <td>7.1            </td> <td>0.61 </td> <td>1.33                        </td> <td>425    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.23  </td> <td>3.3       </td> <td>2.28</td> <td>18.5             </td> <td>98       </td> <td>1.8          </td> <td>0.83      </td> <td>0.61                </td> <td>1.87           </td> <td>10.52          </td> <td>0.56 </td> <td>1.51                        </td> <td>675    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.58  </td> <td>1.29      </td> <td>2.1 </td> <td>20               </td> <td>103      </td> <td>1.48         </td> <td>0.58      </td> <td>0.53                </td> <td>1.4            </td> <td>7.6            </td> <td>0.58 </td> <td>1.55                        </td> <td>640    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.17  </td> <td>5.19      </td> <td>2.32</td> <td>22               </td> <td>93       </td> <td>1.74         </td> <td>0.63      </td> <td>0.61                </td> <td>1.55           </td> <td>7.9            </td> <td>0.6  </td> <td>1.48                        </td> <td>725    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.84  </td> <td>4.12      </td> <td>2.38</td> <td>19.5             </td> <td>89       </td> <td>1.8          </td> <td>0.83      </td> <td>0.48                </td> <td>1.56           </td> <td>9.01           </td> <td>0.57 </td> <td>1.64                        </td> <td>480    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.45  </td> <td>3.03      </td> <td>2.64</td> <td>27               </td> <td>97       </td> <td>1.9          </td> <td>0.58      </td> <td>0.63                </td> <td>1.14           </td> <td>7.5            </td> <td>0.67 </td> <td>1.73                        </td> <td>880    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>14.34  </td> <td>1.68      </td> <td>2.7 </td> <td>25               </td> <td>98       </td> <td>2.8          </td> <td>1.31      </td> <td>0.53                </td> <td>2.7            </td> <td>13             </td> <td>0.57 </td> <td>1.96                        </td> <td>660    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.48  </td> <td>1.67      </td> <td>2.64</td> <td>22.5             </td> <td>89       </td> <td>2.6          </td> <td>1.1       </td> <td>0.52                </td> <td>2.29           </td> <td>11.75          </td> <td>0.57 </td> <td>1.78                        </td> <td>620    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.36  </td> <td>3.83      </td> <td>2.38</td> <td>21               </td> <td>88       </td> <td>2.3          </td> <td>0.92      </td> <td>0.5                 </td> <td>1.04           </td> <td>7.65           </td> <td>0.56 </td> <td>1.58                        </td> <td>520    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.69  </td> <td>3.26      </td> <td>2.54</td> <td>20               </td> <td>107      </td> <td>1.83         </td> <td>0.56      </td> <td>0.5                 </td> <td>0.8            </td> <td>5.88           </td> <td>0.96 </td> <td>1.82                        </td> <td>680    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.85  </td> <td>3.27      </td> <td>2.58</td> <td>22               </td> <td>106      </td> <td>1.65         </td> <td>0.6       </td> <td>0.6                 </td> <td>0.96           </td> <td>5.58           </td> <td>0.87 </td> <td>2.11                        </td> <td>570    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.96  </td> <td>3.45      </td> <td>2.35</td> <td>18.5             </td> <td>106      </td> <td>1.39         </td> <td>0.7       </td> <td>0.4                 </td> <td>0.94           </td> <td>5.28           </td> <td>0.68 </td> <td>1.75                        </td> <td>675    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.78  </td> <td>2.76      </td> <td>2.3 </td> <td>22               </td> <td>90       </td> <td>1.35         </td> <td>0.68      </td> <td>0.41                </td> <td>1.03           </td> <td>9.58           </td> <td>0.7  </td> <td>1.68                        </td> <td>615    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.73  </td> <td>4.36      </td> <td>2.26</td> <td>22.5             </td> <td>88       </td> <td>1.28         </td> <td>0.47      </td> <td>0.52                </td> <td>1.15           </td> <td>6.62           </td> <td>0.78 </td> <td>1.75                        </td> <td>520    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.45  </td> <td>3.7       </td> <td>2.6 </td> <td>23               </td> <td>111      </td> <td>1.7          </td> <td>0.92      </td> <td>0.43                </td> <td>1.46           </td> <td>10.68          </td> <td>0.85 </td> <td>1.56                        </td> <td>695    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.82  </td> <td>3.37      </td> <td>2.3 </td> <td>19.5             </td> <td>88       </td> <td>1.48         </td> <td>0.66      </td> <td>0.4                 </td> <td>0.97           </td> <td>10.26          </td> <td>0.72 </td> <td>1.75                        </td> <td>685    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.58  </td> <td>2.58      </td> <td>2.69</td> <td>24.5             </td> <td>105      </td> <td>1.55         </td> <td>0.84      </td> <td>0.39                </td> <td>1.54           </td> <td>8.66           </td> <td>0.74 </td> <td>1.8                         </td> <td>750    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.4   </td> <td>4.6       </td> <td>2.86</td> <td>25               </td> <td>112      </td> <td>1.98         </td> <td>0.96      </td> <td>0.27                </td> <td>1.11           </td> <td>8.5            </td> <td>0.67 </td> <td>1.92                        </td> <td>630    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.2   </td> <td>3.03      </td> <td>2.32</td> <td>19               </td> <td>96       </td> <td>1.25         </td> <td>0.49      </td> <td>0.4                 </td> <td>0.73           </td> <td>5.5            </td> <td>0.66 </td> <td>1.83                        </td> <td>510    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>12.77  </td> <td>2.39      </td> <td>2.28</td> <td>19.5             </td> <td>86       </td> <td>1.39         </td> <td>0.51      </td> <td>0.48                </td> <td>0.64           </td> <td>9.9            </td> <td>0.57 </td> <td>1.63                        </td> <td>470    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>14.16  </td> <td>2.51      </td> <td>2.48</td> <td>20               </td> <td>91       </td> <td>1.68         </td> <td>0.7       </td> <td>0.44                </td> <td>1.24           </td> <td>9.7            </td> <td>0.62 </td> <td>1.71                        </td> <td>660    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.71  </td> <td>5.65      </td> <td>2.45</td> <td>20.5             </td> <td>95       </td> <td>1.68         </td> <td>0.61      </td> <td>0.52                </td> <td>1.06           </td> <td>7.7            </td> <td>0.64 </td> <td>1.74                        </td> <td>740    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.4   </td> <td>3.91      </td> <td>2.48</td> <td>23               </td> <td>102      </td> <td>1.8          </td> <td>0.75      </td> <td>0.43                </td> <td>1.41           </td> <td>7.3            </td> <td>0.7  </td> <td>1.56                        </td> <td>750    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.27  </td> <td>4.28      </td> <td>2.26</td> <td>20               </td> <td>120      </td> <td>1.59         </td> <td>0.69      </td> <td>0.43                </td> <td>1.35           </td> <td>10.2           </td> <td>0.59 </td> <td>1.56                        </td> <td>835    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>13.17  </td> <td>2.59      </td> <td>2.37</td> <td>20               </td> <td>120      </td> <td>1.65         </td> <td>0.68      </td> <td>0.53                </td> <td>1.46           </td> <td>9.3            </td> <td>0.6  </td> <td>1.62                        </td> <td>840    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0    </td> <td>14.13  </td> <td>4.1       </td> <td>2.74</td> <td>24.5             </td> <td>96       </td> <td>2.05         </td> <td>0.76      </td> <td>0.56                </td> <td>1.35           </td> <td>9.2            </td> <td>0.61 </td> <td>1.6                         </td> <td>560    </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">shuffled_wine</span> <span class="o">=</span> <span class="n">wine1</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">with_replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
<span class="n">trainset</span> <span class="o">=</span> <span class="n">shuffled_wine</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">90</span><span class="p">))</span>
<span class="n">testset</span>  <span class="o">=</span> <span class="n">shuffled_wine</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">91</span><span class="p">,</span> <span class="mi">178</span><span class="p">))</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_accuracy2</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">testset</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.9195402298850575</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t_color</span> <span class="o">=</span> <span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="s1">'Class'</span><span class="p">,</span> <span class="n">make_array</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="s1">'Color'</span><span class="p">,</span> <span class="n">make_array</span><span class="p">(</span><span class="s1">'darkblue'</span><span class="p">,</span> <span class="s1">'gold'</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">t_new</span> <span class="o">=</span> <span class="n">wine1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="n">t_color</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">t_new</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">'Color'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Implementing_Nearest_Neighbor_Classifiers_42_0.png"/></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wine1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">wine1</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Implementing_Nearest_Neighbor_Classifiers_43_0.png"/></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>