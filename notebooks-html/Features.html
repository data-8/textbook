<div id="ipython-notebook">
            <a class="interact-button" href="https://mybinder.org/v2/gh/data-8/textbook/gh-pages?filepath=notebooks/Features.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-a-model">Building a model<a class="anchor-link" href="#Building-a-model">¶</a></h2><p>So far, we have talked about <em>prediction</em>, where the purpose of learning is to be able to predict the class of new instances.  I'm now going to switch to <em>model building</em>, where the goal is to learn a model of how the class depends upon the attributes.</p>
<p>One place where model building is useful is for science: e.g., which genes influence whether you become diabetic?  This is interesting and useful in its right (apart from any applications to predicting whether a particular individual will become diabetic), because it can potentially help us understand the workings of our body.</p>
<p>Another place where model building is useful is for control: e.g., what should I change about my advertisement to get more people to click on it?  How should I change the profile picture I use on an online dating site, to get more people to "swipe right"?  Which attributes make the biggest difference to whether people click/swipe?  Our goal is to determine which attributes to change, to have the biggest possible effect on something we care about.</p>
<p>We already know how to build a classifier, given a training set.  Let's see how to use that as a building block to help us solve these problems.</p>
<p>How do we figure out which attributes have the biggest influence on the output?  Take a moment and see what you can come up with.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Feature-selection">Feature selection<a class="anchor-link" href="#Feature-selection">¶</a></h2><p>Background: attributes are also called <em>features</em>, in the machine learning literature.</p>
<p>Our goal is to find a subset of features that are most relevant to the output.  The way we'll formalize is this is to identify a subset of features that, when we train a classifier using just those features, gives the highest possible accuracy at prediction.</p>
<p>Intuitively, if we get 90% accuracy using all the features and 88% accuracy using just three of the features (for example), then it stands to reason that those three features are probably the most relevant, and they capture most of the information that affects or determines the output.</p>
<p>With this insight, our problem becomes:</p>
<blockquote><p>Find the subset of $\ell$ features that gives the best possible accuracy (when we use only those $\ell$ features for prediction).</p>
</blockquote>
<p>This is a feature selection problem.  There are many possible approaches to feature selection.  One simple one is to try all possible ways of choosing $\ell$ of the features, and evaluate the accuracy of each.  However, this can be very slow, because there are so many ways to choose a subset of $\ell$ features.</p>
<p>Therefore, we'll consider a more efficient procedure that often works reasonably well in practice.  It is known as greedy feature selection.  Here's how it works.</p>
<ol>
<li><p>Suppose there are $d$ features.  Try each on its own, to see how much accuracy we can get using a classifier trained with just that one feature.  Keep the best feature.</p>
</li>
<li><p>Now we have one feature.  Try remaining $d-1$ features, to see which is the best one to add to it (i.e., we are now training a classifier with just 2 features: the best feature picked in step 1, plus one more).  Keep the one that best improves accuracy.  Now we have 2 features.</p>
</li>
<li><p>Repeat.  At each stage, we try all possibilities for how to add one more feature to the feature subset we've already picked, and we keep the one that best improves accuracy.</p>
</li>
</ol>
<p>Let's implement it and try it on some examples!</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Code-for-k-NN">Code for k-NN<a class="anchor-link" href="#Code-for-k-NN">¶</a></h2><p>First, some code from last time, to implement $k$-nearest neighbors.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">pt1</span><span class="p">,</span> <span class="n">pt2</span><span class="p">):</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pt1</span><span class="p">)):</span>
        <span class="n">tot</span> <span class="o">=</span> <span class="n">tot</span> <span class="o">+</span> <span class="p">(</span><span class="n">pt1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pt2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tot</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">rows</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">attributes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">withdists</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">withdists</span><span class="o">.</span><span class="n">append_column</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">,</span> <span class="n">dists</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">withdists</span>

<span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">withdists</span> <span class="o">=</span> <span class="n">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">sortedbydist</span> <span class="o">=</span> <span class="n">withdists</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sortedbydist</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topk</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">&gt;</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">topkclasses</span> <span class="o">=</span> <span class="n">closestk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">validattrs</span> <span class="o">=</span> <span class="n">valid</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="n">numcorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="c1"># Run the classifier on the ith patient in the test set</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validattrs</span><span class="o">.</span><span class="n">rows</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">k</span><span class="p">)</span>
        <span class="c1"># Was the classifier's prediction correct?</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">valid</span><span class="p">[</span><span class="s1">'Class'</span><span class="p">][</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">numcorrect</span> <span class="o">=</span> <span class="n">numcorrect</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">numcorrect</span> <span class="o">/</span> <span class="n">valid</span><span class="o">.</span><span class="n">num_rows</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Code-for-feature-selection">Code for feature selection<a class="anchor-link" href="#Code-for-feature-selection">¶</a></h2><p>Now we'll implement the feature selection algorithm.  First, a subroutine to evaluate the accuracy when using a particular subset of features:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">)</span>
    <span class="n">va</span> <span class="o">=</span> <span class="n">valid</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">va</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we'll implement a subroutine that, given a current subset of features, tries all possible ways to add one more feature to the subset, and evaluates the accuracy of each candidate.  This returns a table that summarizes the accuracy of each option it examined.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">try_one_more_feature</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">baseattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="p">([</span><span class="s1">'Attribute'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">baseattrs</span><span class="p">)</span><span class="o">.</span><span class="n">labels</span><span class="p">:</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="p">[</span><span class="n">attr</span><span class="p">]</span><span class="o">+</span><span class="n">baseattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">attr</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Accuracy'</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we'll implement the greedy feature selection algorithm, using the above subroutines.  For our own purposes of understanding what's going on, I'm going to have it print out, at each iteration, all features it considered and the accuracy it got with each.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">select_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">maxfeatures</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="p">([</span><span class="s1">'NumAttrs'</span><span class="p">,</span> <span class="s1">'Attributes'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">])</span>
    <span class="n">curattrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">maxfeatures</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">curattrs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">iters</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'== Computing best feature to add to '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">curattrs</span><span class="p">))</span>
        <span class="c1"># Try all ways of adding just one more feature to curattrs</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">try_one_more_feature</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">curattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="c1"># Take the single best feature and add it to curattrs</span>
        <span class="n">attr</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">'Attribute'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">curattrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attr</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">curattrs</span><span class="p">),</span> <span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curattrs</span><span class="p">),</span> <span class="n">acc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example:-Tree-Cover">Example: Tree Cover<a class="anchor-link" href="#Example:-Tree-Cover">¶</a></h2><p>Now let's try it out on an example.  I'm working with a data set gathered by the US Forestry service.  They visited thousands of wildnerness locations and recorded various characteristics of the soil and land.  They also recorded what kind of tree was growing predominantly on that land.  Focusing only on areas where the tree cover was either Spruce or Lodgepole Pine, let's see if we can figure out which characteristics have the greatest effect on whether the predominant tree cover is Spruce or Lodgepole Pine.</p>
<p>There are 500,000 records in this data set -- more than I can analyze with the software we're using.  So, I'll pick a random sample of just a fraction of these records, to let us do some experiments that will complete in a reasonable amount of time.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_trees</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">'treecover2.csv.gz'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">','</span><span class="p">)</span>
<span class="n">training</span>   <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span>   <span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
<span class="n">validation</span> <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1500</span><span class="p">))</span>
<span class="n">test</span>       <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">))</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">training</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Elevation</th> <th>Aspect</th> <th>Slope</th> <th>HorizDistToWater</th> <th>VertDistToWater</th> <th>HorizDistToRoad</th> <th>Hillshade9am</th> <th>HillshadeNoon</th> <th>Hillshade3pm</th> <th>HorizDistToFire</th> <th>Area1</th> <th>Area2</th> <th>Area3</th> <th>Area4</th> <th>Class</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>2804     </td> <td>139   </td> <td>9    </td> <td>268             </td> <td>65             </td> <td>3180           </td> <td>234         </td> <td>238          </td> <td>135         </td> <td>6121           </td> <td>1    </td> <td>0    </td> <td>0    </td> <td>0    </td> <td>1    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>2785     </td> <td>155   </td> <td>18   </td> <td>242             </td> <td>118            </td> <td>3090           </td> <td>238         </td> <td>238          </td> <td>122         </td> <td>6211           </td> <td>1    </td> <td>0    </td> <td>0    </td> <td>0    </td> <td>1    </td>
        </tr>
    </tbody>
</table>
<p>... (998 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start by figuring out how accurate a classifier will be, if trained using this data.  I'm going to arbitrarily use $k=15$ for the $k$-nearest neighbor classifier.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.572</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll apply feature selection.  I wonder which characteristics have the biggest influence on whether Spruce vs Lodgepole Pine grows?  We'll look for the best 4 features.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_features</span> <span class="o">=</span> <span class="n">select_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to []
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Elevation       </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad </td> <td>0.562   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire </td> <td>0.534   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am    </td> <td>0.518   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade3pm    </td> <td>0.516   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope           </td> <td>0.516   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4           </td> <td>0.514   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3           </td> <td>0.514   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2           </td> <td>0.514   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1           </td> <td>0.514   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater </td> <td>0.512   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon   </td> <td>0.51    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect          </td> <td>0.51    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToWater</td> <td>0.502   </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to ['Elevation']
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>HorizDistToWater</td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am    </td> <td>0.776   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope           </td> <td>0.77    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade3pm    </td> <td>0.768   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4           </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3           </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2           </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1           </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect          </td> <td>0.75    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon   </td> <td>0.748   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad </td> <td>0.73    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire </td> <td>0.664   </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to ['Elevation', 'HorizDistToWater']
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Hillshade3pm   </td> <td>0.792   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon  </td> <td>0.792   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope          </td> <td>0.784   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect         </td> <td>0.784   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4          </td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3          </td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2          </td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1          </td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am   </td> <td>0.78    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater</td> <td>0.776   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad</td> <td>0.708   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire</td> <td>0.64    </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre></pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_features</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>NumAttrs</th> <th>Attributes</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1       </td> <td>Elevation                                </td> <td>0.762   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>2       </td> <td>Elevation, HorizDistToWater              </td> <td>0.782   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3       </td> <td>Elevation, HorizDistToWater, Hillshade3pm</td> <td>0.792   </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, Elevation looks like far and away the most discriminative feature.  This suggests that this characteristic might play a large role in the biology of which tree grows best, and thus might tell us something about the science.</p>
<p>What about the horizontal distance to water and the amount of shade at 3pm?  It looks like they might also be predictive, and thus might play a role in the biology as well -- assuming the apparent improvement in accuracy is real, and we're not just fooling ourselves.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hold-out-sets:-Training,-Validation,-Testing">Hold-out sets: Training, Validation, Testing<a class="anchor-link" href="#Hold-out-sets:-Training,-Validation,-Testing">¶</a></h2><p>Suppose we built a predictor using just the best two features, Elevation and HorizDistToWater.  How accurate would we expect it to be, on the entire population of locations?  78.2% accurate?  more?  less?  Why?</p>
<p>Well, at this point, it's hard to tell.  It's the same issue we mentioned earlier about fooling ourselves.  We've tried multiple different approaches, and taken the best; if we then evaluate it on the same data set we used to select which is best, we will get a biased numbers -- it might not be an accurate estimate of the true accuracy.</p>
<p>Why?  Our data set is noisy.  We've looked for correlations, and kept the association that had the highest correlation in our training set.  But was that a real relationship, or was it just noise?  If you pick a single attribute and measure its accuracy on a sample, we'd expect this to be a reasonable approximation to its accuracy on the entire population, but with some random error.  If we look at 100 possible combinations and choose the best, it's possible we found one whose accuracy on the entire population is indeed large -- or it's possible we were just selecting the one whose error term happened to be the largest of the 100.</p>
<p>For these reasons, we can't expect the accuracy numbers we've computed above to necessarily be a good, unbiased measure of the accuracy on the entire population.</p>
<p>The way to get an unbiased estimate of accuracy is the same as last lecture: get some more data; or set some aside in the beginning so we have more when we need it.  In this case, I set aside two extra chunks of data, a <em>validation</em> data set and a <em>test</em> data set.  I used the validation set to select a few best features.  Now we're going to measure the performance of this on the test set, just to see what happens.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.762</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.712</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">,</span> <span class="s1">'HorizDistToWater'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.782</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">,</span> <span class="s1">'HorizDistToWater'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.742</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do you think we see this difference?</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To get a better feeling for this, we can look at all of our different features, and compare its accuracy on the training set vs its accuracy on the validation set, to see how much random error there is in these accuracy figures.  If there was no error, then both accuracy numbers would always be the same, but as we'll see, in practice there is some error, because computing the accuracy on a sample is only an approximation to the accuracy on the entire population.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracies</span> <span class="o">=</span> <span class="n">Table</span><span class="p">([</span><span class="s1">'Validation Accuracy'</span><span class="p">,</span> <span class="s1">'Accuracy on another sample'</span><span class="p">])</span>
<span class="n">another_sample</span> <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2500</span><span class="p">))</span>
<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">labels</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">another_sample</span><span class="p">,</span> <span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">accuracies</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Validation Accuracy'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Validation Accuracy</th> <th>Accuracy on another sample</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>0.502              </td> <td>0.346                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.51               </td> <td>0.384                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.51               </td> <td>0.35                      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.512              </td> <td>0.358                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.514              </td> <td>0.342                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.514              </td> <td>0.342                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.514              </td> <td>0.342                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.514              </td> <td>0.342                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.516              </td> <td>0.366                     </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>0.516              </td> <td>0.352                     </td>
        </tr>
    </tbody>
</table>
<p>... (4 rows omitted)</p></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracies</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">'Validation Accuracy'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.4/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if self._edgecolors == str('face'):
</pre></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Features_30_1.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Thought-Questions">Thought Questions<a class="anchor-link" href="#Thought-Questions">¶</a></h2><p>Suppose that the top two attributes had been Elevation and HorizDistToRoad.  Interpret this for me.  What might this mean for the biology of trees?  One possible explanation is that the  distance to the nearest road affects what kind of tree grows; can you give any other possible explanations?</p>
<p>Once we know the top two attributes are Elevation and HorizDistToWater, suppose we next wanted to know <em>how</em> they affect what kind of tree grows: e.g., does high elevation tend to favor spruce, or does it favor lodgepole pine?  How would you go about answering these kinds of questions?</p>
<p>The scientists also gathered some more data that I left out, for simplicity: for each location, they also gathered what kind of soil it has, out of 40 different types.  The original data set had a column for soil type, with numbers from 1-40 indicating which of the 40 types of soil was present.  Suppose I wanted to include this among the other characteristics.  What would go wrong, and how could I fix it up?</p>
<p>For this example we picked $k=15$ arbitrarily.  Suppose we wanted to pick the best value of $k$ -- the one that gives the best accuracy.  How could we go about doing that?  What are the pitfalls, and how could they be addressed?</p>
<p>Suppose I wanted to use feature selection to help me adjust my online dating profile picture to get the most responses.  There are some characteristics I can't change (such as how handsome I am), and some I can (such as whether I smile or not).  How would I adjust the feature selection algorithm above to account for this?</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div></div></div></div>