<div id="ipython-notebook">
            <a class="interact-button" href="http://datahub.berkeley.edu/user-redirect/interact?repo=textbook&path=notebooks/grades_and_piazza.csv&path=notebooks/Multiple.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have explored ways to use multiple features to predict a categorical variable, it is natural to study ways of using multiple predictor variables to predict a quantitative variable. A commonly used method to do this is called <em>multiple regression</em>.</p>
<p>We will start with an example to review some fundamental aspects of <em>simple</em> regression, that is, regression based on one predictor variable.</p>
<h3 id="Example:-Simple-Regression">Example: Simple Regression<a class="anchor-link" href="#Example:-Simple-Regression">¶</a></h3><p>Suppose that our goal is to use regression to estimate the height of a basset hound based on its weight, using a sample that looks consistent with the regression model. Suppose the observed correlation $r$ is 0.5, and that the summary statistics for the two variables are as in the table below:</p>
<table>
<thead><tr>
<th style="text-align:right"></th>
<th style="text-align:center"><strong>average</strong></th>
<th style="text-align:center"><strong>SD</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">height</td>
<td style="text-align:center">14 inches</td>
<td style="text-align:center">2 inches</td>
</tr>
<tr>
<td style="text-align:right">weight</td>
<td style="text-align:center">50 pounds</td>
<td style="text-align:center">5 pounds</td>
</tr>
</tbody>
</table>
<p>To calculate the equation of the regression line, we need the slope and the intercept.</p>
$$
\mbox{slope} ~=~ \frac{r \cdot \mbox{SD of }y}{\mbox{SD of }x} ~=~
\frac{0.5 \cdot 2 \mbox{ inches}}{5 \mbox{ pounds}} ~=~ 0.2 ~\mbox{inches per pound}
$$$$
\mbox{intercept} ~=~ \mbox{average of }y - \mbox{slope}\cdot \mbox{average of } x
~=~ 14 \mbox{ inches} ~-~ 0.2 \mbox{ inches per pound} \cdot 50 \mbox{ pounds}
~=~ 4 \mbox{ inches}
$$<p>The equation of the regression line allows us to calculate the estimated height, in inches,
based on a given weight in pounds:</p>
$$
\mbox{estimated height} ~=~ 0.2 \cdot \mbox{given weight} ~+~ 4
$$<p>The slope of the line is measures the increase in the estimated height per unit increase in weight. The slope is positive, and it is important to note that this does not mean that we think basset hounds get taller if they put on weight. The slope reflects the difference in the average heights of two groups of dogs that are 1 pound apart in weight. Specifically, consider a group of dogs whose weight is $w$ pounds, and the group whose weight is $w+1$ pounds. The second group is estimated to be 0.2 inches taller, on average. This is true for all values of $w$ in the sample.</p>
<p>In general, the slope of the regression line can be interpreted as the average increase in $y$ per unit increase in $x$. Note that if the slope is negative, then for every unit increase in $x$, the average of $y$ decreases.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiple-Predictors">Multiple Predictors<a class="anchor-link" href="#Multiple-Predictors">¶</a></h3><p>In multiple regression, more than one predictor variable is used to estimate $y$. For example, a Dartmouth study of undergraduates collected information about their use of an online course forum and their GPA. We might want to predict a student's GPA based on the number of days that they visit the course forum and how many times they answer someone else's question.  Then the multiple regression model would involve two slopes, an intercept, and random errors as before:</p>
<p>GPA $~=~ \mbox{slope}_d * \mbox{days} ~+~ \mbox{slope}_a * \mbox{answers} ~+~ \mbox{intercept} ~+~ \mbox{random error}$</p>
<p>Our goal would be to find the estimated GPA using the best estimates of the two slopes and the intercept; as before, the "best" estimates are those that minimize the mean squared error of estimation.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To start off, we will investigate the data set, which is <a href="http://studentlife.cs.dartmouth.edu/">described online</a>. Each row represents a student. In addition to the student's GPA, the row tallies</p>
<ul>
<li><code>days online</code>: The number of days on which the student viewed the online forum</li>
<li><code>views</code>: The number of posts viewed</li>
<li><code>contributions</code>: The number of contributions, including posts and follow-up discussions</li>
<li><code>questions</code>: The number of questions posted</li>
<li><code>notes</code>: The number of notes posted</li>
<li><code>answers</code>: The number of answers posted</li>
</ul></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">'grades_and_piazza.csv'</span><span class="p">)</span>
<span class="n">grades</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>GPA</th> <th>days online</th> <th>views</th> <th>contributions</th> <th>questions</th> <th>notes</th> <th>answers</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>2.863</td> <td>29         </td> <td>299  </td> <td>5            </td> <td>1        </td> <td>1    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.505</td> <td>57         </td> <td>299  </td> <td>0            </td> <td>0        </td> <td>0    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.029</td> <td>27         </td> <td>101  </td> <td>1            </td> <td>1        </td> <td>0    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.679</td> <td>67         </td> <td>301  </td> <td>1            </td> <td>0        </td> <td>0    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.474</td> <td>43         </td> <td>201  </td> <td>12           </td> <td>1        </td> <td>0    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.705</td> <td>67         </td> <td>308  </td> <td>45           </td> <td>22       </td> <td>0    </td> <td>5      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.806</td> <td>36         </td> <td>171  </td> <td>20           </td> <td>4        </td> <td>3    </td> <td>4      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.667</td> <td>82         </td> <td>300  </td> <td>26           </td> <td>11       </td> <td>0    </td> <td>3      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.245</td> <td>44         </td> <td>127  </td> <td>6            </td> <td>1        </td> <td>1    </td> <td>0      </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3.293</td> <td>35         </td> <td>259  </td> <td>16           </td> <td>13       </td> <td>1    </td> <td>0      </td>
        </tr>
    </tbody>
</table>
<p>... (20 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Correlation-Matrix">Correlation Matrix<a class="anchor-link" href="#Correlation-Matrix">¶</a></h3><p>Perhaps we wish to predict GPA based on forum usage. A natural first step is to see which variables are correlated with the GPA. Here is the correlation matrix. The <code>to_df</code> method generates a <a href="http://pandas.pydata.org/">Pandas</a> dataframe containing the same data as the table, and its <code>corr</code> method generates a matrix of correlations for each pair of columns.</p>
<p>All forum usage variables are correlated with GPA, but the <code>notes</code> correlation has a very small magnitude.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>GPA</th>
      <th>days online</th>
      <th>views</th>
      <th>contributions</th>
      <th>questions</th>
      <th>notes</th>
      <th>answers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>GPA</th>
      <td>1.000000</td>
      <td>0.684905</td>
      <td>0.444175</td>
      <td>0.427897</td>
      <td>0.409212</td>
      <td>-0.160604</td>
      <td>0.440382</td>
    </tr>
    <tr>
      <th>days online</th>
      <td>0.684905</td>
      <td>1.000000</td>
      <td>0.654557</td>
      <td>0.448319</td>
      <td>0.435269</td>
      <td>-0.230839</td>
      <td>0.502810</td>
    </tr>
    <tr>
      <th>views</th>
      <td>0.444175</td>
      <td>0.654557</td>
      <td>1.000000</td>
      <td>0.426406</td>
      <td>0.361002</td>
      <td>0.065627</td>
      <td>0.365010</td>
    </tr>
    <tr>
      <th>contributions</th>
      <td>0.427897</td>
      <td>0.448319</td>
      <td>0.426406</td>
      <td>1.000000</td>
      <td>0.857981</td>
      <td>0.295661</td>
      <td>0.702679</td>
    </tr>
    <tr>
      <th>questions</th>
      <td>0.409212</td>
      <td>0.435269</td>
      <td>0.361002</td>
      <td>0.857981</td>
      <td>1.000000</td>
      <td>-0.006365</td>
      <td>0.515661</td>
    </tr>
    <tr>
      <th>notes</th>
      <td>-0.160604</td>
      <td>-0.230839</td>
      <td>0.065627</td>
      <td>0.295661</td>
      <td>-0.006365</td>
      <td>1.000000</td>
      <td>0.229599</td>
    </tr>
    <tr>
      <th>answers</th>
      <td>0.440382</td>
      <td>0.502810</td>
      <td>0.365010</td>
      <td>0.702679</td>
      <td>0.515661</td>
      <td>0.229599</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To start off, let us perform the simple regression of <code>GPA</code> on just <code>days online</code>, the count with the strongest correlation with an <code>r</code> of 0.68. Here is the scatter diagram and regression line.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">'days online'</span><span class="p">,</span> <span class="s1">'GPA'</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Multiple_9_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can immediately see that the relation is not entirely linear, and a residual plot highlights this fact. One issue is that the GPA is never above 4.0, so the model assumption that <code>y</code> values are bell shaped does not hold in this case.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">residual_plot</span><span class="p">(</span><span class="n">grades</span><span class="p">,</span> <span class="s1">'days online'</span><span class="p">,</span> <span class="s1">'GPA'</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Multiple_11_0.png"/></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Nonetheless, the regression line does capture some of the patter in the data, and so we will continue to work with it, noting that our predictions may not be entirely justified by the regression model.</p>
<p>The root mean squared error is a bit above 1/4 of a GPA point when predicting GPA from only the days online.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades_days_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">grades</span><span class="p">,</span> <span class="s1">'days online'</span><span class="p">,</span> <span class="s1">'GPA'</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">grades_days_mse</span><span class="p">)</span>
<span class="n">grades_days_mse</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.28494512878662448</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Two-Predictors">Two Predictors<a class="anchor-link" href="#Two-Predictors">¶</a></h3></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Perhaps more information can improve our prediction. The number of <code>answers</code> is also correlated with GPA. When using it alone as a predictor, the root mean squared error is greater because the correlation has lower magnitude than <code>days online</code>.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades_answers_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">grades</span><span class="p">,</span> <span class="s1">'answers'</span><span class="p">,</span> <span class="s1">'GPA'</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">grades_answers_mse</span><span class="p">)</span>
<span class="n">grades_answers_mse</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.35110518920562062</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, the two variables can be used together. First, we define the mean squared error in terms of a line that has two slopes, one for each variable, as well as an intercept.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">grades_days_answers_mse</span><span class="p">(</span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">fitted</span> <span class="o">=</span> <span class="n">a_days</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'days online'</span><span class="p">)</span> <span class="o">+</span> \
             <span class="n">a_answers</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'answers'</span><span class="p">)</span> <span class="o">+</span> \
             <span class="n">b</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'GPA'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">fitted</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">minimize</span><span class="p">(</span><span class="n">grades_days_answers_mse</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>array([ 0.0157683,  0.0301254,  2.6031789])</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>minimize</code> function returns three values, the slope for <code>days online</code>, the slope for <code>answers</code>, and the intercept. Together, these three values describe a linear prediction function. For example, someone who spent 50 days online and answered 5 questions is predicted to have a GPA of about 3.54.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mf">0.0157683</span> <span class="o">*</span> <span class="mi">50</span> <span class="o">+</span> <span class="mf">0.0301254</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="mf">2.6031789</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>3.54</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The root mean squared error of this predictor is a bit smaller than that of either single-variable predictor we had before!</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">grades_days_answers_mse</span><span class="p">)</span>
<span class="n">grades_days_answers_mse</span><span class="p">(</span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.28161529539241298</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Combining-all-variables">Combining all variables<a class="anchor-link" href="#Combining-all-variables">¶</a></h3></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can combine all information about the course forum in order to attempt to find an even better predictor.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">fit_grades</span><span class="p">(</span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_views</span><span class="p">,</span> <span class="n">a_contributions</span><span class="p">,</span> <span class="n">a_questions</span><span class="p">,</span> <span class="n">a_notes</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a_days</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'days online'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">a_views</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'views'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">a_contributions</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'contributions'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">a_questions</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'questions'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">a_notes</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'notes'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">a_answers</span> <span class="o">*</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'answers'</span><span class="p">)</span> <span class="o">+</span> \
           <span class="n">b</span>

<span class="k">def</span> <span class="nf">grades_all_mse</span><span class="p">(</span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_views</span><span class="p">,</span> <span class="n">a_contributions</span><span class="p">,</span> <span class="n">a_questions</span><span class="p">,</span> <span class="n">a_notes</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">fitted</span> <span class="o">=</span> <span class="n">fit_grades</span><span class="p">(</span><span class="n">a_days</span><span class="p">,</span> <span class="n">a_views</span><span class="p">,</span> <span class="n">a_contributions</span><span class="p">,</span> <span class="n">a_questions</span><span class="p">,</span> <span class="n">a_notes</span><span class="p">,</span> <span class="n">a_answers</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'GPA'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">fitted</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">minimize</span><span class="p">(</span><span class="n">grades_all_mse</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>array([  1.43703000e-02,  -8.15000000e-05,   6.50720000e-03,
        -1.72780000e-03,  -4.04014000e-02,   1.59645000e-02,
         2.65375470e+00])</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using all of this information, we have constructed a predictor with an even smaller root mean squared error. The <code>*</code> below passes all of the return values of the <code>minimize</code> function as arguments to <code>grades_all_mse</code>.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grades_all_mse</span><span class="p">(</span><span class="o">*</span><span class="n">minimize</span><span class="p">(</span><span class="n">grades_all_mse</span><span class="p">))</span> <span class="o">**</span> <span class="mf">0.5</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.27783237243108722</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>All of this information did not improve our predictor very much. The root mean squared error decreased from 2.82 for the best single-variable predictor to 2.78 when using all of the available information.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Definition of $R^2$, consistent with our old $r^2$</strong></p>
<p>When we studied simple regression, we had noted that</p>
$$
|r| ~=~ \frac{\mbox{SD of fitted values of }y}{\mbox{SD of observed values of } y}
$$<p>Let us use our old functions to compute the fitted values and confirm that this is true for our example:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fitted</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">grades</span><span class="p">,</span> <span class="s1">'days online'</span><span class="p">,</span> <span class="s1">'GPA'</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">fitted</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'GPA'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.68490480299828194</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because variance is the square of the standard deviation, we can say that</p>
$$
0.469 ~=~r^2 ~=~ \frac{\mbox{variance of fitted values of }y}{\mbox{variance of observed values of }y}
$$</div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that this way of thinking about $r^2$ involves only the estimated values and the observed values, <em>not the number of predictor variables</em>. Therefore, it motivates the definition of <em>multiple $R^2$</em>:</p>
$$
R^2 ~=~ \frac{\mbox{variance of fitted values of }y}{\mbox{variance of observed values of }y}
$$<p>It is a fact of mathematics that this quantity is always between 0 and 1. With multiple predictor variables, there is no clear interpretation of a sign attached to the square root of $R^2$. Some of the predictors might be positively associated with $y$, others negatively. An overall measure of the fit is provided by $R^2$.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fitted</span> <span class="o">=</span> <span class="n">fit_grades</span><span class="p">(</span><span class="o">*</span><span class="n">minimize</span><span class="p">(</span><span class="n">grades_all_mse</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">fitted</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'GPA'</span><span class="p">))</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.49525855565521787</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is not always wise to maximize $R^2$ by including many variables, but we will address this concern later in the text.</p>
<p>We can also view the residual plot of multiple regression, just as before. The horizontal axis shows fitted values and the vertical axis shows errors. Again, there is some structure to this plot, so perhaps the relation between these variables is not entirely linear. In this case, the structure is minor enough that the regression model is a reasonable choice of predictor.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Table</span><span class="p">()</span><span class="o">.</span><span class="n">with_columns</span><span class="p">([</span>
        <span class="s1">'fitted'</span><span class="p">,</span> <span class="n">fitted</span><span class="p">,</span>
        <span class="s1">'errors'</span><span class="p">,</span> <span class="n">grades</span><span class="o">.</span><span class="n">column</span><span class="p">(</span><span class="s1">'GPA'</span><span class="p">)</span> <span class="o">-</span> <span class="n">fitted</span>
    <span class="p">])</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fit_line</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_png output_subarea ">
<img src="/notebooks-images/Multiple_35_0.png"/></div></div>