<div id="ipython-notebook">
            <a class="interact-button" href="http://datahub.berkeley.edu/user-redirect/interact?repo=textbook&path=notebooks/Models.ipynb">Interact</a>
            
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$']],
      processEscapes: true
    }
  });
</script>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Building-a-model">Building a model<a class="anchor-link" href="#Building-a-model">¶</a></h2><p>So far, we have talked about <em>prediction</em>, where the purpose of learning is to be able to predict the class of new instances.  I'm now going to switch to <em>model building</em>, where the goal is to learn a model of how the class depends upon the attributes.</p>
<p>One place where model building is useful is for science: e.g., which genes influence whether you become diabetic?  This is interesting and useful in its right (apart from any applications to predicting whether a particular individual will become diabetic), because it can potentially help us understand the workings of our body.</p>
<p>Another place where model building is useful is for control: e.g., what should I change about my advertisement to get more people to click on it?  How should I change the profile picture I use on an online dating site, to get more people to "swipe right"?  Which attributes make the biggest difference to whether people click/swipe?  Our goal is to determine which attributes to change, to have the biggest possible effect on something we care about.</p>
<p>We already know how to build a classifier, given a training set.  Let's see how to use that as a building block to help us solve these problems.</p>
<p>How do we figure out which attributes have the biggest influence on the output?  Take a moment and see what you can come up with.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Feature-selection">Feature selection<a class="anchor-link" href="#Feature-selection">¶</a></h3><p>Background: attributes are also called <em>features</em>, in the machine learning literature.</p>
<p>Our goal is to find a subset of features that are most relevant to the output.  The way we'll formalize is this is to identify a subset of features that, when we train a classifier using just those features, gives the highest possible accuracy at prediction.</p>
<p>Intuitively, if we get 90% accuracy using the features and 88% accuracy using just three of the features (for example), then it stands to reason that those three features are probably the most relevant, and they capture most of the information that affects or determines the output.</p>
<p>With this insight, our problem becomes:</p>
<blockquote><p>Find the subset of $\ell$ features that gives the best possible accuracy (when we use only those $\ell$ features for prediction).</p>
</blockquote>
<p>This is a feature selection problem.  There are many possible approaches to feature selection.  One simple one is to try all possible ways of choosing $\ell$ of the features, and evaluate the accuracy of each.  However, this can be very slow, because there are so many ways to choose a subset of $\ell$ features.</p>
<p>Therefore, we'll consider a more efficient procedure that often works reasonably well in practice.  It is known as greedy feature selection.  Here's how it works.</p>
<ol>
<li><p>Suppose there are $d$ features.  Try each on its own, to see how much accuracy we can get using a classifier trained with just that one feature.  Keep the best feature.</p>
</li>
<li><p>Now we have one feature.  Try remaining $d-1$ features, to see which is the best one to add to it (i.e., we are now training a classifier with just 2 features: the best feature picked in step 1, plus one more).  Keep the one that best improves accuracy.  Now we have 2 features.</p>
</li>
<li><p>Repeat.  At each stage, we try all possibilities for how to add one more feature to the feature subset we've already picked, and we keep the one that best improves accuracy.</p>
</li>
</ol>
<p>Let's implement it and try it on some examples!</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Code-for-k-NN">Code for k-NN<a class="anchor-link" href="#Code-for-k-NN">¶</a></h3><p>First, some code from last time, to implement $k$-nearest neighbors.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">pt1</span><span class="p">,</span> <span class="n">pt2</span><span class="p">):</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pt1</span><span class="p">)):</span>
        <span class="n">tot</span> <span class="o">=</span> <span class="n">tot</span> <span class="o">+</span> <span class="p">(</span><span class="n">pt1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">pt2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tot</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span><span class="o">.</span><span class="n">rows</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">attributes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">withdists</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">withdists</span><span class="o">.</span><span class="n">append_column</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">,</span> <span class="n">dists</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">withdists</span>

<span class="k">def</span> <span class="nf">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">withdists</span> <span class="o">=</span> <span class="n">computetablewithdists</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">sortedbydist</span> <span class="o">=</span> <span class="n">withdists</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Distance'</span><span class="p">)</span>
    <span class="n">topk</span> <span class="o">=</span> <span class="n">sortedbydist</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topk</span>

<span class="k">def</span> <span class="nf">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">&gt;</span> <span class="n">topkclasses</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">num_rows</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">closestk</span> <span class="o">=</span> <span class="n">closest</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">topkclasses</span> <span class="o">=</span> <span class="n">closestk</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">majority</span><span class="p">(</span><span class="n">topkclasses</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_accuracy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">validattrs</span> <span class="o">=</span> <span class="n">valid</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'Class'</span><span class="p">)</span>
    <span class="n">numcorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid</span><span class="o">.</span><span class="n">num_rows</span><span class="p">):</span>
        <span class="c1"># Run the classifier on the ith patient in the test set</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">classify</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validattrs</span><span class="o">.</span><span class="n">rows</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">k</span><span class="p">)</span>
        <span class="c1"># Was the classifier's prediction correct?</span>
        <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">valid</span><span class="p">[</span><span class="s1">'Class'</span><span class="p">][</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">numcorrect</span> <span class="o">=</span> <span class="n">numcorrect</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">numcorrect</span> <span class="o">/</span> <span class="n">valid</span><span class="o">.</span><span class="n">num_rows</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Code-for-feature-selection">Code for feature selection<a class="anchor-link" href="#Code-for-feature-selection">¶</a></h3><p>Now we'll implement the feature selection algorithm.  First, a subroutine to evaluate the accuracy when using a particular subset of features:</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">tr</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">)</span>
    <span class="n">va</span> <span class="o">=</span> <span class="n">valid</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">va</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we'll implement a subroutine that, given a current subset of features, tries all possible ways to add one more feature to the subset, and evaluates the accuracy of each candidate.  This returns a table that summarizes the accuracy of each option it examined.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">try_one_more_feature</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">baseattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="s1">'Attribute'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">training</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">'Class'</span><span class="p">]</span><span class="o">+</span><span class="n">baseattrs</span><span class="p">)</span><span class="o">.</span><span class="n">column_labels</span><span class="p">:</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="p">[</span><span class="n">attr</span><span class="p">]</span><span class="o">+</span><span class="n">baseattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">attr</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">'Accuracy'</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we'll implement the greedy feature selection algorithm, using the above subroutines.  For our own purposes of understanding what's going on, I'm going to have it print out, at each iteration, all features it considered and the accuracy it got with each.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">select_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">maxfeatures</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="s1">'NumAttrs'</span><span class="p">,</span> <span class="s1">'Attributes'</span><span class="p">,</span> <span class="s1">'Accuracy'</span><span class="p">])</span>
    <span class="n">curattrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">maxfeatures</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">column_labels</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">curattrs</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">iters</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'== Computing best feature to add to '</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">curattrs</span><span class="p">))</span>
        <span class="c1"># Try all ways of adding just one more feature to curattrs</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">try_one_more_feature</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">valid</span><span class="p">,</span> <span class="n">curattrs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">r</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">()</span>
        <span class="c1"># Take the single best feature and add it to curattrs</span>
        <span class="n">attr</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">'Attribute'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="s1">'Accuracy'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">curattrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attr</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">curattrs</span><span class="p">),</span> <span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">curattrs</span><span class="p">),</span> <span class="n">acc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tree-Cover">Tree Cover<a class="anchor-link" href="#Tree-Cover">¶</a></h3><p>Now let's try it out on an example.  I'm working with a data set gathered by the US Forestry service.  They visited thousands of wildnerness locations and recorded various characteristics of the soil and land.  They also recorded what kind of tree was growing predominantly on that land.  Focusing only on areas where the tree cover was either Spruce or Lodgepole Pine, let's see if we can figure out which characteristics have the greatest effect on whether the predominant tree cover is Spruce or Lodgepole Pine.</p>
<p>There are 500,000 records in this data set -- more than I can analyze with the software we're using.  So, I'll pick a random sample of just a fraction of these records, to let us do some experiments that will complete in a reasonable amount of time.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">all_trees</span> <span class="o">=</span> <span class="n">Table</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">'treecover2.csv.gz'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">','</span><span class="p">)</span>
<span class="n">all_trees</span> <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">all_trees</span><span class="o">.</span><span class="n">num_rows</span><span class="p">)</span>
<span class="n">training</span>   <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span>   <span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
<span class="n">validation</span> <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1500</span><span class="p">))</span>
<span class="n">test</span>       <span class="o">=</span> <span class="n">all_trees</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1500</span><span class="p">,</span> <span class="mi">2000</span><span class="p">))</span>
</pre></div></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">training</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Elevation</th> <th>Aspect</th> <th>Slope</th> <th>HorizDistToWater</th> <th>VertDistToWater</th> <th>HorizDistToRoad</th> <th>Hillshade9am</th> <th>HillshadeNoon</th> <th>Hillshade3pm</th> <th>HorizDistToFire</th> <th>Area1</th> <th>Area2</th> <th>Area3</th> <th>Area4</th> <th>Class</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>2990     </td> <td>357   </td> <td>18   </td> <td>696             </td> <td>121            </td> <td>2389           </td> <td>189         </td> <td>205          </td> <td>151         </td> <td>1654           </td> <td>0    </td> <td>0    </td> <td>1    </td> <td>0    </td> <td>1    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3255     </td> <td>283   </td> <td>27   </td> <td>418             </td> <td>149            </td> <td>360            </td> <td>134         </td> <td>228          </td> <td>228         </td> <td>794            </td> <td>0    </td> <td>1    </td> <td>0    </td> <td>0    </td> <td>0    </td>
        </tr>
    </tbody>
</table>
<p>... (998 rows omitted)</p></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's start by figuring out how accurate a classifier will be, if trained using this data.  I'm going to arbitrarily use $k=15$ for the $k$-nearest neighbor classifier.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_accuracy</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.722</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll apply feature selection.  I wonder which characteristics have the biggest influence on whether Spruce vs Lodgepole Pine grows?  We'll look for the best 4 features.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_features</span> <span class="o">=</span> <span class="n">select_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to []
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Elevation       </td> <td>0.746   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2           </td> <td>0.608   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4           </td> <td>0.586   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire </td> <td>0.564   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater </td> <td>0.564   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad </td> <td>0.56    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade3pm    </td> <td>0.554   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect          </td> <td>0.554   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon   </td> <td>0.548   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am    </td> <td>0.548   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToWater</td> <td>0.542   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope           </td> <td>0.538   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3           </td> <td>0.414   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1           </td> <td>0.414   </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to ['Elevation']
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>HorizDistToWater</td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect          </td> <td>0.774   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon   </td> <td>0.772   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad </td> <td>0.772   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am    </td> <td>0.766   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire </td> <td>0.76    </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3           </td> <td>0.756   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1           </td> <td>0.756   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope           </td> <td>0.756   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater </td> <td>0.754   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade3pm    </td> <td>0.752   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4           </td> <td>0.746   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2           </td> <td>0.744   </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>== Computing best feature to add to ['Elevation', 'HorizDistToWater']
</pre></div>
<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>Attribute</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Hillshade3pm   </td> <td>0.788   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HillshadeNoon  </td> <td>0.786   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Slope          </td> <td>0.784   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area4          </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area3          </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area2          </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Area1          </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Hillshade9am   </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>VertDistToWater</td> <td>0.774   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToFire</td> <td>0.756   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>Aspect         </td> <td>0.756   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>HorizDistToRoad</td> <td>0.748   </td>
        </tr>
    </tbody>
</table></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre></pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_features</span>
</pre></div></div></div>
<div class="output_html rendered_html output_subarea output_execute_result">
<table border="1" class="dataframe">
    <thead>
        <tr>
            <th>NumAttrs</th> <th>Attributes</th> <th>Accuracy</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1       </td> <td>Elevation                                </td> <td>0.746   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>2       </td> <td>Elevation, HorizDistToWater              </td> <td>0.778   </td>
        </tr>
    </tbody>
        <tbody><tr>
            <td>3       </td> <td>Elevation, HorizDistToWater, Hillshade3pm</td> <td>0.788   </td>
        </tr>
    </tbody>
</table></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, Elevation looks like far and away the most discriminative feature.  Assuming you have that, it also looks like the distance to water or a road might be a good second feature.  This suggests that these characteristics might play a role in the biology of which tree grows best, and thus might tell us something about the science.</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Hold-out-sets:-Training,-Validation,-Testing">Hold-out sets: Training, Validation, Testing<a class="anchor-link" href="#Hold-out-sets:-Training,-Validation,-Testing">¶</a></h2><p>Suppose we built a predictor using just the best two features, Elevation and HorizDistToWater.  How accurate would we expect it to be, on new locations that we haven't tried yet?  74.6% accurate?  more?  less?  Why?</p>
<p>The correct answer is: the same, or less.  It's the same issue we mentioned last lecture about testing on the training set.  We've tried multiple different approaches, and taken the best; if we then evaluate it on the same data set we used to select which is best, we will get a biased numbers -- an overestimate of the true accuracy.</p>
<p>Why?  Here's an analogy.  Suppose the coach of the track team holds try-outs.  100 students try out, and he has them all run 800 meters and times them.  He picks the fastest student and has them represent us in their next track meet.  How do you think that student's performance in the next track meet will be, compared to their tryout?  Faster than in tryouts?  Slower than in tryouts?  Exactly the same?</p>
<p>Well, if running was all skill and no luck, then the student's time would be exactly the same.  But there's also an element of randomness: some people do better, or worse, on any given day.  If it was all randomness, the coach would be picking the runner who got the luckiest in try-outs, not the runner who is the fastest, and at the meet that runner would almost certainly be slower than in tryouts -- their speed in the tryout is biased, not an accurate estimate of their future performance.  The same will tend to be true if performance is a mixture of skill and luck.</p>
<p>And each combination of features we've tried out is like a runner.  We picked the combination that did the best in our trials, but there's an element of randomness there, so we might just be seeing random fluctuations rather than a combination that's truly better.</p>
<p>The way to get an unbiased estimate of performance is the same as last lecture: get some more data; or set some aside in the beginning so we have more when we need it.  In this case, I set aside two extra chunks of data, a <em>validation</em> data set and a <em>test</em> data set.  I used the validation set to select a few best features.  Now we're going to measure the performance of this on the test set, just to see what happens.</p></div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.746</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">validation</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">,</span> <span class="s1">'HorizDistToWater'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.778</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.724</pre></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">evaluate_features</span><span class="p">(</span><span class="n">training</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="p">[</span><span class="s1">'Elevation'</span><span class="p">,</span> <span class="s1">'HorizDistToWater'</span><span class="p">],</span> <span class="mi">15</span><span class="p">)</span>
</pre></div></div></div>
<div class="output_text output_subarea output_execute_result">
<pre>0.772</pre></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Why do you think we see this difference?</p></div></div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Thought-Questions">Thought Questions<a class="anchor-link" href="#Thought-Questions">¶</a></h3><p>Suppose that the top two attributes had been Elevation and HorizDistToRoad.  Interpret this for me.  What might this mean for the biology of trees?  One possible explanation is that the  distance to the nearest road affects what kind of tree grows; can you give any other possible explanations?</p>
<p>Once we know the top two attributes are Elevation and HorizDistToWater, suppose we next wanted to know <em>how</em> they affect what kind of tree grows: e.g., does high elevation tend to favor spruce, or does it favor lodgepole pine?  How would you go about answering these kinds of questions?</p>
<p>The scientists also gathered some more data that I left out, for simplicity: for each location, they also gathered what kind of soil it has, out of 40 different types.  The original data set had a column for soil type, with numbers from 1-40 indicating which of the 40 types of soil was present.  Suppose I wanted to include this among the other characteristics.  What would go wrong, and how could I fix it up?</p>
<p>For this example we picked $k=15$ arbitrarily.  Suppose we wanted to pick the best value of $k$ -- the one that gives the best accuracy.  How could we go about doing that?  What are the pitfalls, and how could they be addressed?</p>
<p>Suppose I wanted to use feature selection to help me adjust my online dating profile picture to get the most responses.  There are some characteristics I can't change (such as how handsome I am), and some I can (such as whether I smile or not).  How would I adjust the feature selection algorithm above to account for this?</p></div></div></div>